{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stock Sentiment Analysis (Preprocessing)\n",
    "\n",
    "In this example, we are looking at [this Kaggle dataset](https://www.kaggle.com/yash612/stockmarket-sentiment-dataset) and we are borrowing ideas from [this notebook](https://www.kaggle.com/wilk258/stock-text-pyldavis-and-spacy-eda).\n",
    "\n",
    "Our goal is to show how Fugue can be used in the preprocessing step for this NLP problem. Compared with using pandas to do such analysis, Fugue is slightly more complicated, but the advantages are:\n",
    "\n",
    "* Every step, every function is intuitive and easy to understand\n",
    "* The Fugue version is platform and scale agnostic. It can run on any ExecutionEngine and can handle very large dataset that can't fit in one machine.\n",
    "\n",
    "## Install Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**You must restart kernel after installation**\n",
    "\n",
    "## Explore the data\n",
    "\n",
    "We load the data print and do some basic analytics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow as Dag\n",
    "\n",
    "with Dag() as dag:\n",
    "    df = dag.load(\"../../data/stock_sentiment_data.csv\", header=True)\n",
    "    df.show()\n",
    "    dag.select(\"Sentiment, COUNT(*) AS ct FROM\",df, \"GROUP BY Sentiment\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean Up\n",
    "\n",
    "This is based on my personal preference, it's just for demo purposes, I want to:\n",
    "\n",
    "* Make all column names lower cased\n",
    "* Add a unique and deterministic id to each row\n",
    "* Convert sentiment to bool because it has only two values\n",
    "\n",
    "Here I am using a transformer to do it. And I can write the transformer in pure native python. Read [this](../extensions/transformer.ipynb#Schema-Hint) to learn more details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from triad.utils.hash import to_uuid\n",
    "from typing import Iterable, Dict, Any, List\n",
    "\n",
    "# schema: id:str,sentiment:bool,text:str\n",
    "def preprocess(df:Iterable[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        yield dict(id=to_uuid(row[\"Text\"]).split(\"-\")[-1],\n",
    "                   sentiment = str(row[\"Sentiment\"])==\"1\",\n",
    "                   text = row[\"Text\"])\n",
    "        \n",
    "with Dag() as dag:\n",
    "    df = dag.load(\"../../data/stock_sentiment_data.csv\", header=True)\n",
    "    df.transform(preprocess).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize\n",
    "\n",
    "I want to convert the raw text to tokens, so I am going to lower the text, remove punctuations, and stem. These will be done inside a transformer.\n",
    "\n",
    "I feel Iterable as input and output are most intuitive and convenient, so I simply write in the way I like. I also write an additional transformer to convert all data to word sentiment pairs to get some statistics. This is often seen on Spark hello world examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# schema: id:str,sentiment:bool,words:[str]\n",
    "def tokenize(df:Iterable[Dict[str,Any]]) -> Iterable[List[Any]]:\n",
    "    lem=WordNetLemmatizer()\n",
    "    stop=set(stopwords.words('english'))\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for row in df:\n",
    "        words = row[\"text\"].lower().translate(translator).split(\" \")\n",
    "        words = [lem.lemmatize(w) for w in words if w not in stop and w!=\"\"]\n",
    "        yield [row[\"id\"], row[\"sentiment\"], words]\n",
    "        \n",
    "# schema: word:str, sentiment:bool\n",
    "def to_single(df:Iterable[Dict[str,Any]]) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        sentiment = row[\"sentiment\"]\n",
    "        for w in row[\"words\"]:\n",
    "            yield [w,sentiment]\n",
    "\n",
    "with Dag() as dag:\n",
    "    df = dag.load(\"../../data/stock_sentiment_data.csv\", header=True)\n",
    "    tk = df.transform(preprocess).transform(tokenize)\n",
    "    tk.show()\n",
    "    words = tk.transform(to_single)\n",
    "    dag.select(\"word, sentiment, COUNT(*) AS ct FROM\",words,\"GROUP BY word, sentiment ORDER BY ct DESC LIMIT 10\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entity Detection\n",
    "\n",
    "Entity linking can generate very powerful features, so we want to do it for each sentence. And spacy is a popular package to use\n",
    "\n",
    "Entity linking can be very expensive, so making it run distributedly is how we deal with large dataset. In Fugue, it helps you separate the concerns.\n",
    "\n",
    "* Transformer is to handle a partition of data on a single machine, so scalabity and throughput is not the concern of a transformer\n",
    "* How to run a created transformer is associated with scalability and throughput concerns. But again, Fugue is very abstract, you can just tell the system I want to apply the transformation and let the system to optimize the distribution. And actually you can have full control of the distribution, but here we don't go into too much details, let's just focus on *WHAT* intead of *HOW*\n",
    "\n",
    "Another thing to point out is that Fugue avoids the semantic of `map` and only uses `mapPartitions`. `entity_linking` in the following code is a perfect example to demonstrate why. `spacy.load` is expensive, but it's called only once and it can be used for all items in the partition. For the cases you don't have expensive initialization, this approach is neither more complicated nor slower than `map`. So there is no good reason to directly use `map`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "# schema: id:str,sentiment:bool,entities:str\n",
    "def entity_linking(df:Iterable[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    for row in df:\n",
    "        doc = nlp(row[\"text\"])\n",
    "        row[\"entities\"] = json.dumps({str(ent).lower():str(ent.label_) for ent in doc.ents})\n",
    "        yield row\n",
    "        \n",
    "\n",
    "# schema: name:str,label:str,sentiment:bool\n",
    "def to_single_entities(df:Iterable[Dict[str,Any]]) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        for k,v in json.loads(row[\"entities\"]).items():\n",
    "            yield [k,v,row[\"sentiment\"]]\n",
    "        \n",
    "\n",
    "with Dag() as dag:\n",
    "    df = dag.load(\"../../data/stock_sentiment_data.csv\", header=True)\n",
    "    df = dag.select(\"* FROM\",df,\" LIMIT 100\")\n",
    "    pre = df.transform(preprocess).transform(entity_linking)\n",
    "    pre.show()\n",
    "    entities = pre.transform(to_single_entities)\n",
    "    dag.select(\"name, label, sentiment, COUNT(*) AS ct FROM\",entities,\"GROUP BY name, label, sentiment ORDER BY ct DESC LIMIT 10\").show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bring It to Spark!\n",
    "\n",
    "Now it seems both tokenization and entity detection are working well on small data locally. We are going to combine them together. To make it truly scalable, we will use Spark as the execution engine.\n",
    "\n",
    "Pay attention to a few things:\n",
    "\n",
    "* [Auto persist](../advanced/useful_config.ipynb#Auto-Persist) and [parallel run](../advanced/useful_config.ipynb#Parallel-Run) are enabled. So even you write your logic in this simplest way, it will auto persist `df` and `result` because they are used multiple times. Also tokenization and entity linking will run in parallel (if there is enough executors)\n",
    "* This logic can run on any executon engine, you may create an end to end test on small data using `NativeExectuonEngine`\n",
    "* For the transformers it uses, they have no dependency on Fugue.\n",
    "\n",
    "*This step may be slow on binder, if possible, try it with larger data on a real Spark cluster*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue_spark import SparkExecutionEngine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = {\"fugue.workflow.concurrency\":10,\n",
    "        \"fugue.workflow.auto_persist\":True}\n",
    "\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "with Dag(SparkExecutionEngine(session, conf=conf)) as dag:\n",
    "    df = dag.load(\"../../data/stock_sentiment_data.csv\", escape='\"', header=True).transform(preprocess)\n",
    "    tokens = df.transform(tokenize)\n",
    "    entities = df.transform(entity_linking)\n",
    "    result = df.inner_join(tokens,entities)\n",
    "    result.show()\n",
    "    result.save(\"/tmp/stock_sentiment.parquet\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last, let's show an alternative way to describe the end to end logic -- using Fugue SQL. It also adds the steps to print some stats from tokens and entities. In this example, tokens and entities will also be auto persisted because they are also used for multiple times."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "with FugueSQLWorkflow(SparkExecutionEngine(session, conf=conf)) as dag:\n",
    "    dag(\"\"\"\n",
    "    LOAD \"../../data/stock_sentiment_data.csv\"(header=true,escape='\"')\n",
    "    df = TRANSFORM USING preprocess\n",
    "    tokens = TRANSFORM df USING tokenize\n",
    "    entities = TRANSFORM df USING entity_linking\n",
    "    result =\n",
    "        SELECT df.*, words, entities\n",
    "        FROM df \n",
    "        INNER JOIN tokens ON df.id = tokens.id\n",
    "        INNER JOIN entities ON df.id = entities.id\n",
    "    PRINT\n",
    "    SAVE OVERWRITE \"/tmp/stock_sentiment.parquet\"\n",
    "    \n",
    "\n",
    "    SELECT word, sentiment, COUNT(*) AS ct \n",
    "        FROM (TRANSFORM tokens USING to_single)\n",
    "        GROUP BY word, sentiment \n",
    "        ORDER BY ct DESC LIMIT 10\n",
    "    PRINT TITLE \"tokens\"\n",
    "    \n",
    "    SELECT name, label, sentiment, COUNT(*) AS ct\n",
    "        FROM (TRANSFORM entities USING to_single_entities)\n",
    "        GROUP BY name, label, sentiment\n",
    "        ORDER BY ct DESC LIMIT 10\n",
    "    PRINT TITLE \"entities\"\n",
    "    \"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fugue is not only a framework, it's also a way of thinking -- you should keep your code as native as possible and it should be less coupled with any particular computing framework including Fugue itself."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}