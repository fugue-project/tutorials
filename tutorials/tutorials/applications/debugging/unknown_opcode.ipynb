{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SystemError - unknown opcode\n",
    "\n",
    "This normally arises when using Dask on a cluster (it will not happen locally).  The error will look something like this.\n",
    "\n",
    "```\n",
    "SystemError: unknown opcode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common cause is an inconsistent environment between local and the cluster. The local versions of Python, dask, and distributed all need to be aligned because code is serialized with cloudpickle on the local side before it is sent to the cluster. This code is then unpickled and the deserialization will be inconsistent if the Python versions are inconsistent.\n",
    "\n",
    "[This Github issue](https://github.com/dask/distributed/issues/5331) replicates the issue by using a nested function. We can test this is by running the following code on the Dask cluster. When using the code snippet below, configure the Client to point to your Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# insert your client here\n",
    "client = Client()\n",
    "\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from fugue import transform\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def wrap():\n",
    "    # schema: *,x:int\n",
    "    def ppp(df:pd.DataFrame) -> pd.DataFrame:\n",
    "        sleep(2)\n",
    "        return df.assign(x=2)\n",
    "    \n",
    "    n=10000000\n",
    "    df = pd.DataFrame(dict(\n",
    "        a=np.random.rand(n),\n",
    "        b=np.random.rand(n)\n",
    "    ))\n",
    "    ddf = dd.from_pandas(df, npartitions=16)\n",
    "    \n",
    "    return transform(ddf, ppp, engine=\"dask\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give a traceback similar to the following. \n",
    "```\n",
    "SystemError                               Traceback (most recent call last)\n",
    "<ipython-input-6-856eb59918a9> in <module>\n",
    "----> 1 wrap()\n",
    "\n",
    "<ipython-input-5-36bd46412ed1> in wrap()\n",
    "     22     ddf = dd.from_pandas(df, npartitions=16)\n",
    "     23 \n",
    "---> 24     return transform(ddf, ppp, engine=\"dask\").compute()\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/dask/base.py in compute(self, **kwargs)\n",
    "    286         dask.base.compute\n",
    "    287         \"\"\"\n",
    "--> 288         (result,) = compute(self, traverse=False, **kwargs)\n",
    "    289         return result\n",
    "    290 \n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/dask/base.py in compute(*args, **kwargs)\n",
    "    568         postcomputes.append(x.__dask_postcompute__())\n",
    "    569 \n",
    "--> 570     results = schedule(dsk, keys, **kwargs)\n",
    "    571     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n",
    "    572 \n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/distributed/client.py in get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\n",
    "   2691                     should_rejoin = False\n",
    "   2692             try:\n",
    "-> 2693                 results = self.gather(packed, asynchronous=asynchronous, direct=direct)\n",
    "   2694             finally:\n",
    "   2695                 for f in futures.values():\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/distributed/client.py in gather(self, futures, errors, direct, asynchronous)\n",
    "   1967             else:\n",
    "   1968                 local_worker = None\n",
    "-> 1969             return self.sync(\n",
    "   1970                 self._gather,\n",
    "   1971                 futures,\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/distributed/client.py in sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\n",
    "    863             return future\n",
    "    864         else:\n",
    "--> 865             return sync(\n",
    "    866                 self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\n",
    "    867             )\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/distributed/utils.py in sync(loop, func, callback_timeout, *args, **kwargs)\n",
    "    325     if error[0]:\n",
    "    326         typ, exc, tb = error[0]\n",
    "--> 327         raise exc.with_traceback(tb)\n",
    "    328     else:\n",
    "    329         return result[0]\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/distributed/utils.py in f()\n",
    "    308             if callback_timeout is not None:\n",
    "    309                 future = asyncio.wait_for(future, callback_timeout)\n",
    "--> 310             result[0] = yield future\n",
    "    311         except Exception:\n",
    "    312             error[0] = sys.exc_info()\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/tornado/gen.py in run(self)\n",
    "    760 \n",
    "    761                     try:\n",
    "--> 762                         value = future.result()\n",
    "    763                     except Exception:\n",
    "    764                         exc_info = sys.exc_info()\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/distributed/client.py in _gather(self, futures, errors, direct, local_worker)\n",
    "   1832                             exc = CancelledError(key)\n",
    "   1833                         else:\n",
    "-> 1834                             raise exception.with_traceback(traceback)\n",
    "   1835                         raise exc\n",
    "   1836                     if errors == \"skip\":\n",
    "\n",
    "/opt/conda/envs/coiled/lib/python3.8/site-packages/dask/optimization.py in __call__()\n",
    "\n",
    "/opt/conda/envs/coiled/lib/python3.8/site-packages/dask/core.py in get()\n",
    "\n",
    "/opt/conda/envs/coiled/lib/python3.8/site-packages/dask/core.py in _execute_task()\n",
    "\n",
    "/opt/conda/envs/coiled/lib/python3.8/site-packages/dask/utils.py in apply()\n",
    "\n",
    "/opt/conda/envs/coiled/lib/python3.8/site-packages/dask/dataframe/core.py in apply_and_enforce()\n",
    "\n",
    "/usr/local/lib/python3.9/site-packages/fugue_dask/execution_engine.py in _map()\n",
    "    196                 pdf.reset_index(drop=True), input_schema, pandas_df_wrapper=True\n",
    "    197             )\n",
    "--> 198             if on_init_once is not None:\n",
    "    199                 on_init_once(0, input_df)\n",
    "    200             cursor = partition_spec.get_cursor(input_schema, 0)\n",
    "\n",
    "SystemError: unknown opcode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the traceback above, notice there are two versions of Python. Dask requires consistent Python versions between the client and cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
