{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Execution Graph (DAG)\n",
    "\n",
    "This is the most important tutorial you need to read through.\n",
    "\n",
    "The construction of workflow and the construction of ExecutionEngines can be totally decoupled. But you can also couple them, this may be useful when you need to use certain config in an ExecutionEngine. \n",
    "\n",
    "In fugue, workflow is static, it is strictly the description or **spec** of your logic flow, and it has nothing to do with execution. When you finish writing the workflow spec, you can also choose a Fugue ExecutionEngine and an Adagio workflow ExecutionEngine for the end to end execution. The Adagio workflow ExecutionEngine will turn your workflow _spec_ into a workflow _instance_ (nodes specs will also turn to node instances) and execute in certain order. For example Adagio default ExecutionEngine `SequentialExecutionEngine` will run everything sequentially in the order you list in the workflow spec. In each node of the execution graph, it will use the given Fugue ExecutionEngine. And the edges of the graph is always `DataFrame`\n",
    "\n",
    "\n",
    "## Graph Nodes - Extensions\n",
    "\n",
    "There are only 3 types of nodes in Fugue workflow. They are also called driver side extensions.\n",
    "\n",
    "![](../../images/nodes.svg)\n",
    "\n",
    "* **Creator**: no input, single output dataframe, it is to produce dataframe input for other types of nodes, for example load file or create mock data\n",
    "* **Processor**: one or multiple input dataframes, single output dataframe, it is to do certain transformation and pass to the next node\n",
    "* **Outputter**: one or multiple input dataframes, no input, it is to finalize the process of the input, for example save or print\n",
    "\n",
    "All these nodes/extensions are executed on driver side and they are ExecutionEngine aware. For example if you really want to use certain features of RDD, you can write your native Spark code in a `Processor` because inside a processor, you can access the ExecutionEngine and if it is SparkExecutionEngine, you can get `spark_session`. There will be examples in this tutorial.\n",
    "\n",
    "There are two special types of `Processor`: `Transformer` and `CoTransformer`. They are special because they are NOT ExeuctionEngine aware, and their exposed interface is to describe the job to do on worker side not driver side.\n",
    "\n",
    "![](../../images/transformers.svg)\n",
    "\n",
    "* [**Transformer**](./transformer.ipynb): single `LocalDataFrame` in, single `LocalDataFrame` out\n",
    "* [**CoTransformer**](./cotransformer.ipynb): one or multiple `LocalDataFrame` in, single `LocaDataFrame` out\n",
    "* [**OutputTransformer**](./transformer.ipynb#Output-Transformer): single `LocalDataFrame` in, no output\n",
    "* [**OutputCoTransformer**](./cotransformer.ipynb#Output-CoTransformer): one or multiple `LocalDataFrame` in, no output\n",
    "\n",
    "They only care about how to process a partition of the dataframe(s) on a local machine.\n",
    "\n",
    "| . | Creator | Processor | Outputter | Transformer | CoTransformer | OutputTransformer | OutputCoTransformer\n",
    "|---|---|---|---|---|---|---|---\n",
    "|Input | 0    | 1+        | 1+        | 1           | 1+            | 1                 | 1+\n",
    "|Output| 1    | 1         | 0         | 1           | 1             | 0                 | 0\n",
    "|Side  |Driver|Driver     | Driver    | Worker      | Worker        | Worker            | Worker\n",
    "|Engine Aware | Yes | Yes | Yes       | No          | No            | No                | No\n",
    "\n",
    "Fugue orchestrates its extensions in Fugue Workflows\n",
    "\n",
    "\n",
    "## Initialize a Workflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0]],\"a:int\")\n",
    "df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, nothing printed, this is because you only described what you want to do but you didn't really execute the dag. To run it:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dag.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to make it run automatically, use `with` statement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0]],\"a:int\")\n",
    "    df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, where did I setup the ExecutionEngine? So actually by default, `FugueWorkflow` will use `NativeExecutionEngine`, in order to setup your own ExecutionEngine, you can do:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0]],\"a:int\")\n",
    "df.show()\n",
    "\n",
    "# see all the expensive initialization is after the dag is constructed, the steps above are fast\n",
    "# if you have any obvious issue, they will fail fast, \n",
    "# they are compile time problems and the below is all about runtime\n",
    "spark_session = (SparkSession\n",
    "                 .builder\n",
    "                 .config(\"spark.executor.cores\",4)\n",
    "                 .getOrCreate())\n",
    "\n",
    "my_engine = SparkExecutionEngine(spark_session)\n",
    "dag.run(my_engine)\n",
    "\n",
    "# You can also do the following, but you move the dag construction to be after building spark session\n",
    "# so at least you have to wait until the spark session is initialized, you can know if there is compile problems\n",
    "with FugueWorkflow(my_engine) as dag:\n",
    "    df = dag.df([[0]],\"a:int\")\n",
    "    df.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dataframes & Basic operations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow, ArrayDataFrame\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "# You can pass in raw data, pandas dataframe or Fugue DataFrames\n",
    "dag.df([[0]],\"a:int\").show(title=\"from raw data\")\n",
    "dag.df(pd.DataFrame([[0]], columns=[\"a\"])).show(title=\"from pandas\")\n",
    "dag.df(ArrayDataFrame([[0]],\"a:int\")).show(title=\"from Fugue DataFrame\")\n",
    "\n",
    "# basic operations\n",
    "df=dag.df(ArrayDataFrame([[0,1,2]],\"a:int,b:int,c:int\"))\n",
    "df[[\"c\",\"b\"]].show(title=\"select colummns\")\n",
    "df.rename(b=\"d\").rename({\"a\":\"aa\"}).show(title=\"rename\")\n",
    "df.drop([\"c\",\"x\"], if_exists=True).show(title=\"drop columns (if exists)\")\n",
    "df.persist().broadcast().show(title=\"persist and broadcast\")\n",
    "\n",
    "dag.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create DataFrame with Creator\n",
    "\n",
    "Please read [creator tutorial](./creator.ipynb) for details how to write creators in different ways. Here is just one way."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow\n",
    "from typing import List, Any\n",
    "\n",
    "#schema: a:int\n",
    "def create_single(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(create_single).show()\n",
    "    dag.create(create_single, params={\"n\":2}).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process DataFrame with Processor\n",
    "\n",
    "Please read [processor tutorial](./processor.ipynb) for details how to write processor in different ways"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, ExecutionEngine, DataFrame, ArrayDataFrame\n",
    "from typing import List, Any\n",
    "\n",
    "def ct(e:ExecutionEngine, df1:DataFrame, df2:DataFrame) -> DataFrame:\n",
    "    # if the function signature has ExecutionEngine, the creator becomes engine aware\n",
    "    # here we need engine to persist the dataframes before counting\n",
    "    c1, c2 = e.persist(df1).count(), e.persist(df2).count()\n",
    "    return ArrayDataFrame([[c1,c2]],\"c1:long,c2:long\")\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    a=dag.df([[0]],\"a:int\")\n",
    "    b=dag.df([[1],[3]],\"a:int\")\n",
    "    dag.process(a,b,using=ct).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output DataFrame with Outputter\n",
    "\n",
    "Please read [outputter tutorial](./outputter.ipynb) for details how to write outputter in different ways"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, ExecutionEngine, DataFrame, ArrayDataFrame\n",
    "from typing import List, Any\n",
    "\n",
    "def print_array(dfs:DataFrames) -> None:\n",
    "    for df in dfs.values():\n",
    "        print(df.as_array())\n",
    "    print(\"---\")\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    a=dag.df([[0]],\"a:int\")\n",
    "    b=dag.df([[1],[3]],\"a:int\")\n",
    "    dag.output(a,b,using=print_array)\n",
    "    # you can also directly call output from a dag dataframe\n",
    "    a.output(print_array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform & CoTransform\n",
    "\n",
    "Please read [transformer tutorial](./transformer.ipynb) and [cotransformer tutorial](./cotransformer.ipynb) for details how to write transformers in different ways\n",
    "\n",
    "### Transformer\n",
    "In this example we want to get nth smallest or largest row of each partition of column `a`. As you can see transformer is often used after `partition` especially when you want to partition by some keys."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, DataFrame, ArrayDataFrame\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=20000) -> pd.DataFrame:\n",
    "    return pd.DataFrame(np.random.randint(0,100,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "# schema: *\n",
    "def nth(df:Iterable[List[Any]], n) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        if n==0:\n",
    "            yield row\n",
    "            return\n",
    "        n-=1\n",
    "        \n",
    "dag = FugueWorkflow()\n",
    "df = dag.create(helper).persist() # add persist here so the data will not be regenerated\n",
    "largest_4 = df.partition(by=[\"a\"],presort=\"b DESC, c DESC\").transform(nth, params={\"n\":4})\n",
    "smallest_4 = df.partition(by=[\"a\"],presort=\"b,c\").transform(nth, params={\"n\":4})\n",
    "largest_4.persist().show()\n",
    "smallest_4.persist().show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1)) # using native python\n",
    "\n",
    "engine = SparkExecutionEngine(conf={\"fugue.spark.use_pandas_udf\":False})\n",
    "print(timeit(lambda: dag.run(engine), number=1)) # if engine is well configured, this logic can handle very large dataset\n",
    "\n",
    "engine = SparkExecutionEngine(conf={\"fugue.spark.use_pandas_udf\":True})\n",
    "print(timeit(lambda: dag.run(engine), number=1)) # if engine is well configured, this logic can handle very large dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, on such small data, Spark local mode is way slower than running on `NativeExecutionEngine`. In practice, when we unit test Fugue code, try to use `NativeExecutionEngine` since their behavior should be consistent. This is guaranteed on Fugue level, you only choose the faster one in a certain scenario.\n",
    "\n",
    "### CoTransformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, DataFrame, ArrayDataFrame, DataFrames\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=20) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,4,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "# schema: value:[str]\n",
    "def to_str(df1:List[List[Any]], df2:List[List[Any]]) -> List[List[Any]]:\n",
    "    return [[[df1.__repr__(),df2.__repr__()]]]\n",
    "        \n",
    "dag = FugueWorkflow()\n",
    "df1 = dag.create(helper)\n",
    "df2 = dag.create(helper)\n",
    "# you must zip the dataframes and then use cotransformer\n",
    "df1.zip(df2,how=\"inner\",partition={\"by\":[\"a\"],\"presort\":\"b,c\"}).transform(to_str).show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))\n",
    "\n",
    "engine = SparkExecutionEngine()\n",
    "print(timeit(lambda: dag.run(engine), number=1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Pandas UDF on SparkExecutionEngine\n",
    "\n",
    "If you don't know pandas UDF, read [this](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html). With PyArrow and pandas, Spark is able to accelerate certain operations.\n",
    "\n",
    "In Spark 3.0 it also starts to support [some type annotations](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html). But Fugue is more flexibile on type annotations. Besides `pd.DataFrame` you can also use other annotations including `List` and `Iterable`, etc.\n",
    "\n",
    "For certain cases, no matter what input type you specify, we can see great performance gain. But to maximize the gain, it's suggested to use `pd.DataFrame` as the input and output to remove conversion overhead. By doing this, it may hurt the performance on other ExecutionEngines, or SparkExecutionEngine without pandas_udf support. So you need to understand the pros and cons. The best way is to experiment and decide.\n",
    "\n",
    "In Fugue, only when all of the following are satisfied, it uses `pandas_udf`, otherwise, it will fall back to the common way.\n",
    "* config **fugue.spark.use_pandas_udf** is set to true\n",
    "* `partition_spec` has to have non empty partition keys\n",
    "* output schema can't have nested types\n",
    "\n",
    "Plus, this is environment variable must be set on driver and all executors\n",
    "```\n",
    "ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "```\n",
    "otherwise errors will be thrown."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, DataFrame, ArrayDataFrame, DataFrames\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=2000000) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 2)), columns=list('ab'))\n",
    "\n",
    "# schema: a:int,b:double\n",
    "def median(df:pd.DataFrame) -> List[List[Any]]:\n",
    "    b = df[\"b\"].median()\n",
    "    return [[float(df.loc[0,\"a\"]), float(b)]]\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "dag.create(helper).partition(by=\"a\").transform(median).show(title=\"pandas.median\")\n",
    "\n",
    "engine = SparkExecutionEngine() # normal way\n",
    "print(timeit(lambda: dag.run(engine), number=1))\n",
    "\n",
    "engine = SparkExecutionEngine(conf={\"fugue.spark.use_pandas_udf\":True}) # use pandas_udf in the workflow\n",
    "print(timeit(lambda: dag.run(engine), number=1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SELECT Query\n",
    "\n",
    "Firstly, please read [SQLEngine](./execution_engine.ipynb#SQLEngine) to understand the concept. Notice that in this abraction layer, there is no [FugueSQL](./sql.ipynb), the select statement must be acceptable by the specified SQLEngine.\n",
    "\n",
    "[FugueSQL](./sql.ipynb) will use this feature, but it's way more than that."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, SqliteEngine\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "a=dag.df([[0,1],[1,2]],\"a:long,b:long\")\n",
    "b=dag.df([[1,1],[2,2]],\"a:long,c:long\")\n",
    "\n",
    "# see how the dependency are represented in the select function\n",
    "dag.select(\"SELECT * FROM\",a).show() # if you directly use \"SELECT * FROM a\", it will not be able to identify the dependency and will throw error\n",
    "dag.select(\"SELECT * FROM\",b,\"WHERE c=2\").show()\n",
    "dag.select(\"SELECT a.*,c FROM\",a,\" AS a INNER JOIN\",b,\" AS b ON a.a=b.a\").show()\n",
    "\n",
    "# Force using SqliteEngine regardless ExecutionEngine\n",
    "dag.select(\"SELECT a.*,c FROM\",a,\" AS a INNER JOIN\",b,\" AS b ON a.a=b.a\", sql_engine=SqliteEngine).show(title=\"Force using SqliteEngine regardless ExecutionEngine\")\n",
    "\n",
    "\n",
    "dag.run()\n",
    "dag.run(SparkExecutionEngine)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Join\n",
    "\n",
    "Join operation can be done without using SQL. It is a highly optimized operation for any computing framework. And in many cases, you only need join and using SQL is unnecessary, so we separate it as a standalone method.\n",
    "\n",
    "Please also read [this](./execution_engine.ipynb#Join) to see what are supported"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, SqliteEngine\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "a=dag.df([[0,1],[1,2]],\"a:long,b:long\")\n",
    "b=dag.df([[1,1],[2,2]],\"a:long,b:long\")\n",
    "\n",
    "# you can't directly join them only on a because b will be a conflict\n",
    "# so we can rename and join\n",
    "a.join(b.rename({\"b\":\"c\"}),how=\"inner\").show()\n",
    "\n",
    "dag.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, SqliteEngine\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "a=dag.df([[0,1],[1,2]],\"a:long,b:long\")\n",
    "b=dag.df([[1,1],[2,2]],\"a:long,c:long\")\n",
    "c=dag.df([[1,10],[2,20]],\"a:long,d:long\")\n",
    "d=dag.df([[3,1],[3,2]],\"e:long,f:long\")\n",
    "\n",
    "# all supported join types also have correspondent methods\n",
    "a.left_semi_join(b).show()\n",
    "a.left_anti_join(b).show()\n",
    "a.inner_join(b).show()\n",
    "a.left_outer_join(b).show()\n",
    "a.right_outer_join(b).show()\n",
    "a.full_outer_join(b).show()\n",
    "a.cross_join(d).show()\n",
    "\n",
    "# you can join multiple dataframes if no conflict\n",
    "dag.join(a,b,c,how=\"inner\").show()\n",
    "\n",
    "dag.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NOTICE:** Join can have different outcomes between SQL and pandas. Fugue follows the SQL behavior."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    a=dag.df([[None,1],[\"a\",2]],\"a:str,b:long\")\n",
    "    b=dag.df([[None,1],[\"a\",2]],\"a:str,b:long\")\n",
    "    a.inner_join(b).show() # None,1 is excluded"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UNION, INTERSECT, EXCEPT\n",
    "\n",
    "Dataframe set operations can have different outcomes between SQL and pandas. Fugue follows the SQL behavior."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    a=dag.df([[0,1],[1,2]],\"a:long,b:long\")\n",
    "    b=dag.df([[0,1],[0,1],[2,2]],\"a:long,b:long\")\n",
    "    a.union(b).show() # UNION\n",
    "    a.union(b, distinct=False).show() # UNION ALL\n",
    "    a.intersect(b).show() # INTERSECT DISTINCT\n",
    "    a.subtract(b).show() # EXCEPT DISTINCT"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save & Load\n",
    "\n",
    "You may also read [this](./execution_engine.ipynb#Load-&-Save), but the code here is how you will use in practice."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue_spark import SparkExecutionEngine\n",
    "from fugue import ArrayDataFrame, FugueWorkflow\n",
    "from triad.collections.fs import FileSystem\n",
    "\n",
    "df = ArrayDataFrame([[\"1\",\"a\"],[\"1\",\"b\"],[\"2\",\"b\"],[\"2\",\"c\"]],\"a:str,b:str\")\n",
    "\n",
    "# simplest examples\n",
    "dag = FugueWorkflow()\n",
    "dag.df(df).save(\"/tmp/t1.parquet\", mode=\"overwrite\")\n",
    "dag.df(df).save(\"/tmp/t1.csv\", mode=\"overwrite\", header=True)\n",
    "dag.df(df).save(\"/tmp/t1.json\", mode=\"overwrite\")\n",
    "\n",
    "dag.df(df).save(\"/tmp/t2.parquet\", mode=\"overwrite\", single=True)\n",
    "dag.df(df).save(\"/tmp/t2.csv\", mode=\"overwrite\", single=True, header=True)\n",
    "# dag.df(df).save(\"/tmp/t2.json\", mode=\"overwrite\", single=True) # TODO this is currently not supported on Spark\n",
    "\n",
    "dag.run(SparkExecutionEngine)\n",
    "\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "dag.load(\"/tmp/t1.parquet\").show()\n",
    "dag.load(\"/tmp/t1.csv\", header=True).show()\n",
    "dag.load(\"/tmp/t1.csv\", header=True, columns=\"a:int,b:str\").show() # do type conversion when loading\n",
    "dag.load(\"/tmp/t1.csv\", header=True, columns=[\"b\"]).show()\n",
    "\n",
    "dag.load(\"/tmp/t2.parquet\").show()\n",
    "dag.load(\"/tmp/t2.csv\",header=True).show()\n",
    "# dag.load(\"/tmp/t2.json\").show()  # TODO this is currently not supported on Spark\n",
    "\n",
    "dag.run(SparkExecutionEngine)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Persist & Broadcast\n",
    "\n",
    "Any dataframe in the dag can be persisted and broadcasted. But of course you have to do them for a good reason. Broadcasting a very large dataframe is a bad idea, persisting every datafra,e is also a bad idea.\n",
    "\n",
    "In the following code, both `a` and `b` are used multiple times and they are small. So we can consider persisting and broadcasting them.\n",
    "\n",
    "Please also read [this](./execution_engine.ipynb#Persist-&-Broadcast)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fugue import FugueWorkflow, SqliteEngine\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "a=dag.df([[0,1],[1,2]],\"a:long,b:long\").persist()\n",
    "b=dag.df([[1,1],[2,2]],\"a:long,c:long\").persist().broadcast()\n",
    "\n",
    "dag.select(\"SELECT * FROM\",a).show()\n",
    "dag.select(\"SELECT * FROM\",b,\"WHERE c=2\").show()\n",
    "dag.select(\"SELECT a.*,c FROM\",a,\" AS a INNER JOIN\",b,\" AS b ON a.a=b.a\").show()\n",
    "\n",
    "dag.run(SparkExecutionEngine)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}