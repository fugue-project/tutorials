{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fugue Configurations\n",
    "\n",
    "| Config | Default | Description |\n",
    "| :--- | :---: | :--- |\n",
    "| **fugue.workflow.concurrency** | `1` | Max number of tasks that can run in parallel in a DAG (if they do not depend on each other) |\n",
    "| **fugue.spark.use_pandas_udf** | `False` | Automatically use pandas udf for `groupBY apply` semantic, see [details](#Use-Pandas-UDF-on-SparkExecutionEngine) |\n",
    "| **fugue.sql.compile.ignore_case** | `False` | When this is `True`, keywords in FugueSQL will be case insensitive |\n",
    "| **fugue.rpc.server** | [NativeRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.NativeRPCServer) | Full path to a sublcass of [RPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.RPCServer) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Run\n",
    "\n",
    "**Notice: you may not see the expected performance on binder, it's recommended to run this tutorial on docker on a multiple core machine to get decent performance**\n",
    "\n",
    "Still looking at this same example with persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine())\n",
    "a=dag.df([[0]],\"a:int\").transform(tf).persist()\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step `b` and `c` are not dependent on each other, why they can't run in parallel? In native Spark approach, because it can't foresee the following steps, so it can't automate it. But in Fugue, because it's DAG, we have the context of both previous and following steps, so this is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine(conf={\"fugue.workflow.concurrency\":10}))\n",
    "a=dag.df([[0]],\"a:int\").transform(tf).persist()\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fugue.workflow.concurrency** means at any time in max how many DAG tasks can run in parallel. When you set it to `<=1`, the execution is sequential based on your python code order, otherwise, it will parallelize all possible steps.\n",
    "\n",
    "This config normally should be turned on for distributed frameworks, but should be turned off for `NativeExecutionEngine`, because when you use `NativeExecutionEngine`, you want to validate or debug certain piece of your code, so running it on single thread, the error message and your output can be much easier to understand.\n",
    "\n",
    "So compare with the original example, we can reduce the total run time to half without changing your logic but only by changing configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine(\n",
    "    conf={\"fugue.workflow.concurrency\":10,\n",
    "          \"fugue.workflow.auto_persist\":True}))\n",
    "a=dag.df([[0]],\"a:int\").transform(tf)\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pandas UDF on SparkExecutionEngine\n",
    "\n",
    "**Notice: you may not see the expected performance on binder, it's recommended to run this tutorial on docker on a multiple core machine to get decent performance**\n",
    "\n",
    "If you don't know pandas UDF, read [this](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html). With PyArrow and pandas, Spark is able to accelerate certain operations.\n",
    "\n",
    "In Spark 3.0 it also starts to support [some type annotations](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html). But Fugue is more flexibile on type annotations. Besides `pd.DataFrame` you can also use other annotations including `List` and `Iterable`, etc.\n",
    "\n",
    "For certain cases, no matter what input type you specify, we can see great performance gain. But to maximize the gain, it's suggested to use `pd.DataFrame` as the input and output to remove conversion overhead. By doing this, it may hurt the performance on other ExecutionEngines, or SparkExecutionEngine without pandas_udf support. So you need to understand the pros and cons. The best way is to experiment and decide.\n",
    "\n",
    "In Fugue, only when all of the following are satisfied, it uses `pandas_udf`, otherwise, it will fall back to the common way.\n",
    "\n",
    "* config **fugue.spark.use_pandas_udf** is set to true\n",
    "* `partition_spec` has to have non empty partition keys\n",
    "* output schema can't have nested types\n",
    "\n",
    "Plus, for **pyspark < 3** this environment variable must be set on driver and all executors:\n",
    "```\n",
    "ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "```\n",
    "otherwise errors will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config('spark.ui.showConsoleProgress', 'false')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, DataFrame, ArrayDataFrame, DataFrames\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=2000000) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 2)), columns=list('ab'))\n",
    "\n",
    "# schema: a:int,b:double\n",
    "def median(df:pd.DataFrame) -> List[List[Any]]:\n",
    "    b = df[\"b\"].median()\n",
    "    return [[df.loc[0,\"a\"], b]]\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "dag.create(helper).partition(by=\"a\").transform(median).show(title=\"pandas.median\")\n",
    "\n",
    "engine = SparkExecutionEngine() # normal way\n",
    "print(timeit(lambda: dag.run(engine), number=1))\n",
    "\n",
    "engine = SparkExecutionEngine(conf={\"fugue.spark.use_pandas_udf\":True}) # use pandas_udf in the workflow\n",
    "print(timeit(lambda: dag.run(engine), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore Case in Fugue SQL\n",
    "\n",
    "Normally, when writing Fugue SQL, you upper case keywords by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "from fugue import NativeExecutionEngine\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0]] SCHEMA a:int\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can turn on **fugue.sql.compile.ignore_case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow(NativeExecutionEngine(conf={\"fugue.sql.compile.ignore_case\":True})) as dag:\n",
    "    dag(\"\"\"\n",
    "    create [[0]] schema a:int\n",
    "    print\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can make the sql less readable and make you less aware of syntax abiguity or errors, but it may be handy if you want to migrate other sqls into fugue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPCServer settings\n",
    "\n",
    "If you do not have any callbacks in your workflow, don't set this config.\n",
    "\n",
    "For testing callbacks on local machine, don't set this config. [NativeRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.NativeRPCServer) Will be used.\n",
    "\n",
    "Only when you use a distributed execution engine, and you want to use callbacks, set to a server that is distributable.\n",
    "\n",
    "[FlaskRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.flask.FlaskRPCServer) can be used with a distributed execution engine. Unless you have special needs, you just need to follow the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"fugue.rpc.server\": \"fugue.rpc.flask.FlaskRPCServer\",\n",
    "    \"fugue.rpc.flask_server.host\": \"0.0.0.0\",\n",
    "    \"fugue.rpc.flask_server.port\": \"1234\",\n",
    "    \"fugue.rpc.flask_server.timeout\": \"2 sec\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `fugue.rpc.flask.FlaskRPCServer`, you must set `fugue.rpc.flask_server.host` and `fugue.rpc.flask_server.port`, and it's suggested to also set `fugue.rpc.flask_server.timeout` to a reasonable timeout for your own case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pandas UDF on SparkExecutionEngine\n",
    "\n",
    "If you don't know pandas UDF, read [this](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html). With PyArrow and pandas, Spark is able to accelerate certain operations.\n",
    "\n",
    "In Spark 3.0 it also starts to support [some type annotations](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html). But Fugue is more flexibile on type annotations. Besides `pd.DataFrame` you can also use other annotations including `List` and `Iterable`, etc.\n",
    "\n",
    "For certain cases, no matter what input type you specify, we can see great performance gain. But to maximize the gain, it's suggested to use `pd.DataFrame` as the input and output to remove conversion overhead. By doing this, it may hurt the performance on other ExecutionEngines, or SparkExecutionEngine without pandas_udf support. So you need to understand the pros and cons. The best way is to experiment and decide.\n",
    "\n",
    "In Fugue, only when all of the following are satisfied, it uses `pandas_udf`, otherwise, it will fall back to the common way.\n",
    "* config **fugue.spark.use_pandas_udf** is set to true\n",
    "* `partition_spec` has to have non empty partition keys\n",
    "* output schema can't have nested types\n",
    "\n",
    "Plus, this is environment variable must be set on driver and all executors\n",
    "```\n",
    "ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "```\n",
    "otherwise errors will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, DataFrame, ArrayDataFrame, DataFrames\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=2000000) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 2)), columns=list('ab'))\n",
    "\n",
    "# schema: a:int,b:double\n",
    "def median(df:pd.DataFrame) -> List[List[Any]]:\n",
    "    b = df[\"b\"].median()\n",
    "    return [[float(df.loc[0,\"a\"]), float(b)]]\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "dag.create(helper).partition(by=\"a\").transform(median).show(title=\"pandas.median\")\n",
    "\n",
    "engine = SparkExecutionEngine() # normal way\n",
    "print(timeit(lambda: dag.run(engine), number=1))\n",
    "\n",
    "engine = SparkExecutionEngine(conf={\"fugue.spark.use_pandas_udf\":True}) # use pandas_udf in the workflow\n",
    "print(timeit(lambda: dag.run(engine), number=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
