{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fugue Configurations\n",
    "\n",
    "Have questions? Chat with us on Github or Slack:\n",
    "\n",
    "[![Homepage](https://img.shields.io/badge/fugue-source--code-red?logo=github)](https://github.com/fugue-project/fugue)\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "| Config | Default | Description |\n",
    "| :--- | :---: | :--- |\n",
    "| **fugue.spark.use_pandas_udf** | `True` | Automatically use pandas udf for `groupBY apply` semantic, see [details](#Use-Pandas-UDF-on-SparkExecutionEngine) |\n",
    "| **fugue.sql.compile.ignore_case** | `False` | When this is `True`, keywords in FugueSQL will be case insensitive |\n",
    "| **fugue.rpc.server** | [NativeRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.NativeRPCServer) | Full path to a sublcass of [RPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.RPCServer) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pandas UDF on SparkExecutionEngine\n",
    "\n",
    "**Notice: you may not see the expected performance on binder, it's recommended to run this tutorial on docker on a multiple core machine to get decent performance**\n",
    "\n",
    "If you don't know pandas UDF, read [this](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html). With PyArrow and pandas, Spark is able to accelerate certain operations.\n",
    "\n",
    "In Spark 3.0 it also starts to support [some type annotations](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html). But Fugue is more flexibile on type annotations. Besides `pd.DataFrame` you can also use other annotations including `List` and `Iterable`, etc.\n",
    "\n",
    "For certain cases, no matter what input type you specify, we can see great performance gain. But to maximize the gain, it's suggested to use `pd.DataFrame` as the input and output to remove conversion overhead. By doing this, it may hurt the performance on other execution engines, or on Spark without pandas_udf support. So you need to understand the pros and cons. The best way is to experiment and decide.\n",
    "\n",
    "In Fugue, only when all of the following are satisfied, it uses `pandas_udf`, otherwise, it will fall back to the common way.\n",
    "\n",
    "* config **fugue.spark.use_pandas_udf** is set to True (default)\n",
    "* `partition_spec` has to have non empty partition keys\n",
    "* output schema can't have nested types\n",
    "* Spark config **spark.sql.execution.arrow.pyspark.enabled** is set to `\"true\"`\n",
    "\n",
    "Plus, for **pyspark < 3** this environment variable must be set on driver and all executors:\n",
    "```\n",
    "ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "```\n",
    "otherwise errors will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/03 00:16:30 WARN TaskSetManager: Stage 6 contains a task of very large size (3910 KiB). The maximum recommended task size is 1000 KiB.\n",
      "pandas.median\n",
      "SparkDataFrame\n",
      "a:int|b:double\n",
      "-----+--------\n",
      "2    |4.0     \n",
      "9    |4.0     \n",
      "3    |4.0     \n",
      "7    |5.0     \n",
      "4    |5.0     \n",
      "1.0500444139999985\n",
      "23/01/03 00:16:31 WARN TaskSetManager: Stage 9 contains a task of very large size (3910 KiB). The maximum recommended task size is 1000 KiB.\n",
      "pandas.median\n",
      "SparkDataFrame\n",
      "a:int|b:double\n",
      "-----+--------\n",
      "2    |4.0     \n",
      "9    |4.0     \n",
      "3    |4.0     \n",
      "7    |5.0     \n",
      "4    |5.0     \n",
      "0.9417272339999982\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "import fugue.api as fa\n",
    "\n",
    "def helper(ct=2000000) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 2)), columns=list('ab'))\n",
    "\n",
    "# schema: a:int,b:double\n",
    "def median(df:pd.DataFrame) -> List[List[Any]]:\n",
    "    b = df[\"b\"].median()\n",
    "    return [[df.loc[0,\"a\"], b]]\n",
    "\n",
    "def run(engine, conf=None):\n",
    "    with fa.engine_context(engine):\n",
    "        res = fa.transform(helper(), \n",
    "                           median,\n",
    "                           partition={\"by\": \"a\"}, \n",
    "                           engine_conf=conf\n",
    "                           )\n",
    "        fa.show(res, 5, title=\"pandas.median\");\n",
    "\n",
    "print(timeit(lambda: run(spark), number=1))\n",
    "\n",
    "conf = {\"fugue.spark.use_pandas_udf\":True}\n",
    "print(timeit(lambda: run(spark, conf=conf), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore Case in Fugue SQL\n",
    "\n",
    "Normally, when writing FugueSQL, you upper case keywords by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayDataFrame\n",
      "a:int\n",
      "-----\n",
      "0    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue.api import fugue_sql_flow\n",
    "\n",
    "fugue_sql_flow(\"\"\"\n",
    "               CREATE [[0]] SCHEMA a:int\n",
    "               PRINT\n",
    "               \"\"\").run();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can turn pass `fsql_ignore_case=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayDataFrame\n",
      "a:int\n",
      "-----\n",
      "0    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fugue_sql_flow(\"\"\"\n",
    "               create [[0]] schema a:int\n",
    "               print\n",
    "               \"\"\", fsql_ignore_case=True).run();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can make the sql less readable and make you less aware of syntax abiguity or errors, but it may be handy if you want to migrate other SQL queries into FugueSQL.\n",
    "\n",
    "If there are many `fugue_sql_flow` calls, it might be easier to set `fugue.sql.compile.ignore_case` on the execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPCServer settings\n",
    "\n",
    "If you do not have any callbacks in your workflow, don't set this config.\n",
    "\n",
    "For testing callbacks on local machine, don't set this config. [NativeRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.NativeRPCServer) Will be used.\n",
    "\n",
    "Only when you use a distributed execution engine, and you want to use callbacks, set to a server that is distributable.\n",
    "\n",
    "[FlaskRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.flask.FlaskRPCServer) can be used with a distributed execution engine. Unless you have special needs, you just need to follow the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"fugue.rpc.server\": \"fugue.rpc.flask.FlaskRPCServer\",\n",
    "    \"fugue.rpc.flask_server.host\": \"0.0.0.0\",\n",
    "    \"fugue.rpc.flask_server.port\": \"1234\",\n",
    "    \"fugue.rpc.flask_server.timeout\": \"2 sec\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `fugue.rpc.flask.FlaskRPCServer`, you must set `fugue.rpc.flask_server.host` and `fugue.rpc.flask_server.port`, and it's suggested to also set `fugue.rpc.flask_server.timeout` to a reasonable timeout for your own case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
