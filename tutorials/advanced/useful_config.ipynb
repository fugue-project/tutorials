{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fugue Configurations\n",
    "\n",
    "| Config | Default | Description |\n",
    "| :--- | :---: | :--- |\n",
    "| **fugue.workflow.auto_persist** | `False` | If to auto persist outputs used by multiple following steps |\n",
    "| **fugue.workflow.auto_persist_value** | None | Parameter for auto persist |\n",
    "| **fugue.workflow.concurrency** | `1` | Max number of tasks that can run in parallel in a DAG (if they do not depend on each other) |\n",
    "| **fugue.spark.use_pandas_udf** | `False` | Automatically use pandas udf for `groupBY apply` semantic, see [details](#Use-Pandas-UDF-on-SparkExecutionEngine) |\n",
    "| **fugue.sql.compile.ignore_case** | `False` | When this is `True`, keywords in FugueSQL will be case insensitive |\n",
    "| **fugue.rpc.server** | [NativeRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.NativeRPCServer) | Full path to a sublcass of [RPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.RPCServer) |\n",
    "\n",
    "## Auto Persist\n",
    "\n",
    "**Notice: you may not see the expected performance on binder, it's recommended to run this tutorial on docker on a multiple core machine to get decent performance**\n",
    "\n",
    "Let's see an example first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, LocalDataFrame\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "from time import sleep\n",
    "from timeit import timeit\n",
    "\n",
    "# schema: *\n",
    "def tf(df:LocalDataFrame, sec=5) -> LocalDataFrame:\n",
    "    sleep(sec)\n",
    "    return df\n",
    "\n",
    "\n",
    "dag = FugueWorkflow(SparkExecutionEngine())\n",
    "a=dag.df([[0]],\"a:int\").transform(tf)\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, it runs for 20+ seconds on Spark. Acutally, the execution order is a -> b -> a -> c. This is because Spark is lazy, it will materialize only if there is an [action](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions). And when there is an action, all [transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) before the action will run. If the transformation takes 5 hours not 5 seconds, you can imagine how much time wasted. \n",
    "\n",
    "To make it a -> b -> c, 15 seconds, you can persist `a` (there can be a bit overhead to run spark, so maybe ~ 16 sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine())\n",
    "a=dag.df([[0]],\"a:int\").transform(tf).persist()\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be annoying to some people because sometimes you add the second transform `c` later on and forget to add persist to `a`. Whenever you change the code, you need to go through the whole logic again to make sure persists are added appropriately. \n",
    "\n",
    "Fugue has a config to turn on auto persist. When it is `True`, whichever output that is used multiple times, will be persisted automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine(conf={\"fugue.workflow.auto_persist\":True}))\n",
    "a=dag.df([[0]],\"a:int\").transform(tf)\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More over, you can set how to auto persist. For example, I want to use [MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine(\n",
    "    conf={\"fugue.workflow.auto_persist\":True,\n",
    "          \"fugue.workflow.auto_persist_value\":\"MEMORY_ONLY\"}))\n",
    "a=dag.df([[0]],\"a:int\").transform(tf)\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why not set auto persist True as default?\n",
    "\n",
    "* Being explicit is great\n",
    "* Fugue has not implemented auto unpersist, so if you have a lot of steps, the memory usage can be high. But in practice this is very unlikely.\n",
    "* Sometimes you want `a` to be recalculated for certain reasons.\n",
    "\n",
    "That being said, for many users, you can try to make this a default config. Those are edge cases for advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Run\n",
    "\n",
    "**Notice: you may not see the expected performance on binder, it's recommended to run this tutorial on docker on a multiple core machine to get decent performance**\n",
    "\n",
    "Still looking at this same example with persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine())\n",
    "a=dag.df([[0]],\"a:int\").transform(tf).persist()\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step `b` and `c` are not dependent on each other, why they can't run in parallel? In native Spark approach, because it can't foresee the following steps, so it can't automate it. But in Fugue, because it's DAG, we have the context of both previous and following steps, so this is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine(conf={\"fugue.workflow.concurrency\":10}))\n",
    "a=dag.df([[0]],\"a:int\").transform(tf).persist()\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fugue.workflow.concurrency** means at any time in max how many DAG tasks can run in parallel. When you set it to `<=1`, the execution is sequential based on your python code order, otherwise, it will parallelize all possible steps.\n",
    "\n",
    "This config normally should be turned on for distributed frameworks, but should be turned off for `NativeExecutionEngine`, because when you use `NativeExecutionEngine`, you want to validate or debug certain piece of your code, so running it on single thread, the error message and your output can be much easier to understand.\n",
    "\n",
    "So compare with the original example, we can reduce the total run time to half without changing your logic but only by changing configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = FugueWorkflow(SparkExecutionEngine(\n",
    "    conf={\"fugue.workflow.concurrency\":10,\n",
    "          \"fugue.workflow.auto_persist\":True}))\n",
    "a=dag.df([[0]],\"a:int\").transform(tf)\n",
    "b=a.transform(tf)\n",
    "b.show()\n",
    "c=a.transform(tf)\n",
    "c.show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pandas UDF on SparkExecutionEngine\n",
    "\n",
    "**Notice: you may not see the expected performance on binder, it's recommended to run this tutorial on docker on a multiple core machine to get decent performance**\n",
    "\n",
    "If you don't know pandas UDF, read [this](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html). With PyArrow and pandas, Spark is able to accelerate certain operations.\n",
    "\n",
    "In Spark 3.0 it also starts to support [some type annotations](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html). But Fugue is more flexibile on type annotations. Besides `pd.DataFrame` you can also use other annotations including `List` and `Iterable`, etc.\n",
    "\n",
    "For certain cases, no matter what input type you specify, we can see great performance gain. But to maximize the gain, it's suggested to use `pd.DataFrame` as the input and output to remove conversion overhead. By doing this, it may hurt the performance on other ExecutionEngines, or SparkExecutionEngine without pandas_udf support. So you need to understand the pros and cons. The best way is to experiment and decide.\n",
    "\n",
    "In Fugue, only when all of the following are satisfied, it uses `pandas_udf`, otherwise, it will fall back to the common way.\n",
    "\n",
    "* config **fugue.spark.use_pandas_udf** is set to true\n",
    "* `partition_spec` has to have non empty partition keys\n",
    "* output schema can't have nested types\n",
    "\n",
    "Plus, for **pyspark < 3** this environment variable must be set on driver and all executors:\n",
    "```\n",
    "ARROW_PRE_0_15_IPC_FORMAT=1\n",
    "```\n",
    "otherwise errors will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, DataFrame, ArrayDataFrame, DataFrames\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=2000000) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 2)), columns=list('ab'))\n",
    "\n",
    "# schema: a:int,b:double\n",
    "def median(df:pd.DataFrame) -> List[List[Any]]:\n",
    "    b = df[\"b\"].median()\n",
    "    return [[df.loc[0,\"a\"], b]]\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "dag.create(helper).partition(by=\"a\").transform(median).show(title=\"pandas.median\")\n",
    "\n",
    "engine = SparkExecutionEngine() # normal way\n",
    "print(timeit(lambda: dag.run(engine), number=1))\n",
    "\n",
    "engine = SparkExecutionEngine(conf={\"fugue.spark.use_pandas_udf\":True}) # use pandas_udf in the workflow\n",
    "print(timeit(lambda: dag.run(engine), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore Case in Fugue SQL\n",
    "\n",
    "Normally, when writing Fugue SQL, you upper case keywords by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "from fugue import NativeExecutionEngine\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0]] SCHEMA a:int\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can turn on **fugue.sql.compile.ignore_case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow(NativeExecutionEngine(conf={\"fugue.sql.compile.ignore_case\":True})) as dag:\n",
    "    dag(\"\"\"\n",
    "    create [[0]] schema a:int\n",
    "    print\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can make the sql less readable and make you less aware of syntax abiguity or errors, but it may be handy if you want to migrate other sqls into fugue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPCServer settings\n",
    "\n",
    "If you do not have any callbacks in your workflow, don't set this config.\n",
    "\n",
    "For testing callbacks on local machine, don't set this config. [NativeRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.NativeRPCServer) Will be used.\n",
    "\n",
    "Only when you use a distributed execution engine, and you want to use callbacks, set to a server that is distributable.\n",
    "\n",
    "[FlaskRPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.flask.FlaskRPCServer) can be used with a distributed execution engine. Unless you have special needs, you just need to follow the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"fugue.rpc.server\": \"fugue.rpc.flask.FlaskRPCServer\",\n",
    "    \"fugue.rpc.flask_server.host\": \"0.0.0.0\",\n",
    "    \"fugue.rpc.flask_server.port\": \"1234\",\n",
    "    \"fugue.rpc.flask_server.timeout\": \"2 sec\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `fugue.rpc.flask.FlaskRPCServer`, you must set `fugue.rpc.flask_server.host` and `fugue.rpc.flask_server.port`, and it's suggested to also set `fugue.rpc.flask_server.timeout` to a reasonable timeout for your own case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
