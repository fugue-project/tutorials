{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "\n",
    "Data validation is having checks in place to make sure that data comes in the format and specifications that we expect. As data pipelines become more interconnected, the chance of changes unintentionally breaking other pipelines also increases. Validations are used to guarantee that upstream changes will not break the integrity of downstream data operations. They serve as a contract for the requirements that data collection or transformation must uphold.\n",
    "\n",
    "Common data validation patterns include checking for NULL values or checking DataFrame shape to ensure transformations don’t drop any records. Other frequently used operations are checking for column existence and schema. Using data validation avoids silent failures of data processes where everything will run successfully but provide inaccurate results. \n",
    "\n",
    "There are a couple things that using Fugue provides when it comes to validation:\n",
    "* Allows validation code to be reused for both Pandas and Spark projects\n",
    "* Ability to use familiar Pandas-based libraries on Spark\n",
    "* Simple interface for validation on each partition of data\n",
    "\n",
    "To illustrate this, we'll use a simple example with the following Pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>Orlando</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FL</td>\n",
       "      <td>Miami</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FL</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state           city  price\n",
       "0    FL        Orlando      8\n",
       "1    FL          Miami     12\n",
       "2    FL          Tampa     10\n",
       "3    CA  San Francisco     16\n",
       "4    CA    Los Angeles     20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.DataFrame({'state': ['FL','FL','FL','CA','CA','CA'], \n",
    "                     'city': ['Orlando', 'Miami', 'Tampa',\n",
    "                              'San Francisco', 'Los Angeles', 'San Diego'],\n",
    "                     'price': [8, 12, 10, 16, 20, 18]})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pandera for Data Validation\n",
    "\n",
    "Data Validation can be placed at the start of the data pipeline to make sure that any transformations happen smoothly, and it can also be placed at the end to make sure everything is working well before output gets committed to the database. [Pandera](https://github.com/pandera-dev/pandera) is a data validation framework that has a lightweight and expressive syntax, making it good for this demo. The process here will also work for other data validation frameworks as long as their classes can be pickled.\n",
    "\n",
    "For the above DataFrame, we want to guarantee that the price is within a certain range. We want to make sure that the `price` column is at least 8 and not more than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>Orlando</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FL</td>\n",
       "      <td>Miami</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FL</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CA</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state           city  price\n",
       "0    FL        Orlando      8\n",
       "1    FL          Miami     12\n",
       "2    FL          Tampa     10\n",
       "3    CA  San Francisco     16\n",
       "4    CA    Los Angeles     20\n",
       "5    CA      San Diego     18"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandera as pa\n",
    "\n",
    "price_check = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=8,max_value=20)),\n",
    "})\n",
    "\n",
    "def price_validation(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    price_check.validate(data)\n",
    "    return data\n",
    "\n",
    "price_validation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we are using pandera's `DataFrameSchema` to create a validation schema. In the `price_check` variable, we have a `Check` that is applied to a `Column` named price. That validation guarantees that the prices are within an acceptable range of values. We don't need to wrap the validation inside a `price_validation()` function, but this will make bringing the validation to Spark seamless.\n",
    "\n",
    "We highly suggest checking the [Pandera documentation](https://pandera.readthedocs.io/en/stable/) for more information. If you want to see Pandera in action, change the `min_value` or `max_value` in the code above to trigger an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandera on Spark\n",
    "\n",
    "[Pandera](https://github.com/pandera-dev/pandera) now has support for Spark and Dask through PySpark Pandas and Modin, but these have more overhead as they require conversions of the DataFrame objects. Fugue, on the other hand, will pass the validation to the underlying Spark or Dask DataFrames. There are also other use cases enabled by Fugue such as validation per partition seen later.\n",
    "\n",
    "Below is an example of bringing Pandera validations to Spark with minimal code changes.\n",
    "\n",
    "*Note that pyspark needs to be installed in order for the code snippet below to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----+\n",
      "|state|         city|price|\n",
      "+-----+-------------+-----+\n",
      "|   FL|      Orlando|    8|\n",
      "|   FL|        Miami|   12|\n",
      "|   FL|        Tampa|   10|\n",
      "|   CA|San Francisco|   16|\n",
      "|   CA|  Los Angeles|   20|\n",
      "|   CA|    San Diego|   18|\n",
      "+-----+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from fugue import transform\n",
    "\n",
    "price_check = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=5,max_value=20)),\n",
    "})\n",
    "\n",
    "def price_validation(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    price_check.validate(data)\n",
    "    return data\n",
    "\n",
    "# df is a Spark DataFrame\n",
    "df = transform(data, price_validation, schema=\"*\", engine=spark)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no code changes to bring the code to Spark. We only needed to use the fugue `transform()` function. This function ports the Pandas based code reads this and checks to see if the schema is upheld. If users move away from Fugue, the validation function is exactly the same.\n",
    "\n",
    "The `transform()` function is the only addition to bring it to Spark. We use the Fugue `transform()` method to apply the function and because we passed `\"spark\"` as the engine to the `transform()` function, this code will now run in Spark. The `transform()` call will convert the input pandas DataFrame to a Spark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation by Partition with Fugue\n",
    "\n",
    "There is one current shortcoming of the current data validation frameworks. For the data we have, the price ranges of CA and FL are drastically different. Because the validation is applied per column, we don’t have a way to specify different price ranges for each location. It would be ideal however if we could apply a different check for each group of data. This is what we call **validation by partition**. In pandas semantics, it would be the equivalent of `groupby-validate`.\n",
    "\n",
    "This operation becomes very trivial to perform with Fugue. In the above example, we want to apply a different validation for the data in FL and the data in CA. On average, the CA data points have a higher price so we want to create two validation rules depending on the `state`. We do this in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----+\n",
      "|state|         city|price|\n",
      "+-----+-------------+-----+\n",
      "|   CA|San Francisco|   16|\n",
      "|   CA|  Los Angeles|   20|\n",
      "|   CA|    San Diego|   18|\n",
      "|   FL|      Orlando|    8|\n",
      "|   FL|        Miami|   12|\n",
      "|   FL|        Tampa|   10|\n",
      "+-----+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_check_FL = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=7,max_value=13)),\n",
    "})\n",
    "\n",
    "price_check_CA = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=15,max_value=21)),\n",
    "})\n",
    "\n",
    "price_checks = {'CA': price_check_CA, 'FL': price_check_FL}\n",
    "\n",
    "def price_validation(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    location = df['state'].iloc[0]\n",
    "    check = price_checks[location]\n",
    "    check.validate(df)\n",
    "    return df\n",
    "\n",
    "# df is a Spark DataFrame\n",
    "df = transform(data, price_validation, schema=\"*\", engine=\"spark\", partition={\"by\": \"state\"})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above should already look familiar by now. All we did was create two different Pandera DataFrameSchema objects. After that, we modified the `price_validation()` function to pull the location from the DataFrame and apply the appropriate validation. There are two states in our original DataFrame: CA and FL. However, when the data enters the `price_validation()` function, it is already partitioned by the state because of the `partition({\"by\":\"state\"})` passed into `transform()`. This means the function is applied twice: one for FL and once for CA.\n",
    "\n",
    "Here, we are taking advantage of the SparkExecutionEngine by distributing the task across multiple partitions. We partition the data by `state`, and then apply different rules depending on the `state`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning Errors\n",
    "\n",
    "Pandera will raise a `SchemaError` by default that gets buried by the Spark error messaged. To return the errors as a DataFrame, we use can use the following approach. If there are no errors in the data, it will just return an empty DataFrame.\n",
    "\n",
    "To keep the errors for each partition, you can attach the partition key as a column in the returned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+----------------+------------+------------+-----+\n",
      "|schema_context|column|           check|check_number|failure_case|index|\n",
      "+--------------+------+----------------+------------+------------+-----+\n",
      "|        Column| price|in_range(12, 18)|           0|           8|    0|\n",
      "|        Column| price|in_range(12, 18)|           0|          10|    0|\n",
      "|        Column| price|in_range(12, 18)|           0|          20|    0|\n",
      "+--------------+------+----------------+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_schema = \"schema_context:str, column:str, check:str, check_number:int, failure_case:int, index:int\"\n",
    "out_columns = [\"schema_context\", \"column\", \"check\", \"check_number\", \"failure_case\", \"index\"]\n",
    "\n",
    "price_check = pa.DataFrameSchema({\n",
    "    \"price\": pa.Column(pa.Int, pa.Check.in_range(min_value=12,max_value=18)),\n",
    "})\n",
    "\n",
    "def price_validation(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        price_check.validate(data, lazy=True)\n",
    "        return pd.DataFrame(columns=out_columns)\n",
    "    except pa.errors.SchemaErrors as err:\n",
    "        return err.failure_cases\n",
    "\n",
    "transform(data.copy(), price_validation, schema=out_schema)\n",
    "# Use the Spark engine\n",
    "transform(data.copy(), price_validation, schema=out_schema, engine=spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo we showed how Fugue allows Pandas-based data validation frameworks to be used in Spark. This is helpful for organizations that find themselves implementing validation rules twice to support Spark and Pandas implementations. Even though we demoed with Pandera here, this will work with other data validation libraries as long as the classes are serializeable.\n",
    "\n",
    "Fugue also allows users to perform **validation by paritition**, a missing feature in the current data validation frameworks. When dealing with big data, there are normally logical groupings that require slightly different validation rules. Fugue helps partition the data and parallelize applying different rules through Spark or Dask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
