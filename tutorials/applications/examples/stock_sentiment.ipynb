{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Sentiment Analysis (Preprocessing)\n",
    "\n",
    "Have questions? Chat with us on Github or Slack:\n",
    "\n",
    "[![Homepage](https://img.shields.io/badge/fugue-source--code-red?logo=github)](https://github.com/fugue-project/fugue)\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "In this example, we are looking at [this Kaggle dataset](https://www.kaggle.com/yash612/stockmarket-sentiment-dataset) and we are borrowing ideas from [this notebook](https://www.kaggle.com/wilk258/stock-text-pyldavis-and-spacy-eda).\n",
    "\n",
    "Our goal is to show how Fugue can be used in the preprocessing step for this NLP problem. Compared with using pandas to do such analysis, Fugue is slightly more complicated, but the advantages are:\n",
    "\n",
    "* Every step, every function is intuitive and easy to understand\n",
    "* The Fugue version is platform and scale agnostic. It can run on any ExecutionEngine and can handle very large dataset that can't fit in one machine.\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from nltk) (2022.7.25)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from nltk) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kevinkho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinkho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (2.3.7)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (7.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Using cached en_core_web_sm-2.3.1-py3-none-any.whl\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.64.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.8)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.28.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.8)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.23.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/fugue/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.11)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You must restart kernel after installation**\n",
    "\n",
    "## Explore the data\n",
    "\n",
    "We load the data print and do some basic analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "Text:str                                                                              |Sentiment:str\n",
      "--------------------------------------------------------------------------------------+-------------\n",
      "Kickers on my watchlist XIDE TIT SOQ PNK CPW BPZ AJ  trade method 1 or method 2, see p|1            \n",
      "rev posts                                                                             |             \n",
      "user: AAP MOVIE. 55% return for the FEA/GEED indicator just 15 trades for the year.  A|1            \n",
      "WESOME.                                                                               |             \n",
      "user I'd be afraid to short AMZN - they are looking like a near-monopoly in eBooks and|1            \n",
      " infrastructure-as-a-service                                                          |             \n",
      "MNTA Over 12.00                                                                       |1            \n",
      "OI  Over 21.37                                                                        |1            \n",
      "PGNX  Over 3.04                                                                       |1            \n",
      "AAP - user if so then the current downtrend will break. Otherwise just a short-term co|-1           \n",
      "rrection in med-term downtrend.                                                       |             \n",
      "Monday's relative weakness. NYX WIN TIE TAP ICE INT BMC AON C CHK BIIB                |-1           \n",
      "GOOG - ower trend line channel test & volume support.                                 |1            \n",
      "AAP will watch tomorrow for ONG entry.                                                |1            \n",
      "PandasDataFrame\n",
      "Sentiment:str|ct:long\n",
      "-------------+-------\n",
      "-1           |2106   \n",
      "1            |3685   \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fugue.api as fa \n",
    "\n",
    "with fa.engine_context():\n",
    "    df = fa.load(\"../../../data/stock_sentiment_data.csv\", header=True)\n",
    "    fa.show(df)\n",
    "    fa.show(fa.raw_sql(\"SELECT Sentiment, COUNT(*) AS ct FROM\",df, \"GROUP BY Sentiment\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Here, we will:\n",
    "\n",
    "* Make all column names lower cased\n",
    "* Add a unique and deterministic id to each row\n",
    "* Convert sentiment to bool because it has only two values\n",
    "\n",
    "Here we use transformer to do it. And I can write the transformer in pure native python. Read [this](../../beginner/schema.ipynb#schema-hint) to learn more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "id:str   |sentiment:bool|text:str                                                               \n",
      "---------+--------------+-----------------------------------------------------------------------\n",
      "4102bd2ab|True          |Kickers on my watchlist XIDE TIT SOQ PNK CPW BPZ AJ  trade method 1 or \n",
      "986      |              |method 2, see prev posts                                               \n",
      "c0b462af2|True          |user: AAP MOVIE. 55% return for the FEA/GEED indicator just 15 trades f\n",
      "eb6      |              |or the year.  AWESOME.                                                 \n",
      "30464f4d4|True          |user I'd be afraid to short AMZN - they are looking like a near-monopol\n",
      "31a      |              |y in eBooks and infrastructure-as-a-service                            \n",
      "9e7926685|True          |MNTA Over 12.00                                                        \n",
      "732      |              |                                                                       \n",
      "473e64d5b|True          |OI  Over 21.37                                                         \n",
      "792      |              |                                                                       \n",
      "541c485a0|True          |PGNX  Over 3.04                                                        \n",
      "00b      |              |                                                                       \n",
      "4c6d3d0d2|False         |AAP - user if so then the current downtrend will break. Otherwise just \n",
      "13f      |              |a short-term correction in med-term downtrend.                         \n",
      "116c35bfc|False         |Monday's relative weakness. NYX WIN TIE TAP ICE INT BMC AON C CHK BIIB \n",
      "f78      |              |                                                                       \n",
      "5f8bc8dec|True          |GOOG - ower trend line channel test & volume support.                  \n",
      "421      |              |                                                                       \n",
      "e8ee0aade|True          |AAP will watch tomorrow for ONG entry.                                 \n",
      "6f5      |              |                                                                       \n"
     ]
    }
   ],
   "source": [
    "from triad.utils.hash import to_uuid\n",
    "from typing import Iterable, Dict, Any, List\n",
    "\n",
    "# schema: id:str,sentiment:bool,text:str\n",
    "def preprocess(df:Iterable[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        yield dict(id=to_uuid(row[\"Text\"]).split(\"-\")[-1],\n",
    "                   sentiment = str(row[\"Sentiment\"])==\"1\",\n",
    "                   text = row[\"Text\"])\n",
    "        \n",
    "with fa.engine_context():\n",
    "    df = fa.load(\"../../../data/stock_sentiment_data.csv\", header=True)\n",
    "    fa.show(fa.transform(df, preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We will convert the raw text to tokens, so we will to lower the text, remove punctuations, and stem. These will be done inside a transformer.\n",
    "\n",
    "For this case, `Iterable` as input and output are most intuitive and convenient. We will also write an additional transformer to convert all data to word sentiment pairs to get some statistics. This is often seen on Spark hello world examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "id:str   |sentiment:bool|words:[str]                                                             \n",
      "---------+--------------+------------------------------------------------------------------------\n",
      "4102bd2ab|True          |['kicker', 'watchlist', 'xide', 'tit', 'soq', 'pnk', 'cpw', 'bpz', 'aj',\n",
      "986      |              | 'trade', 'method', '1', 'method', '2', 'see', 'prev', 'post']          \n",
      "c0b462af2|True          |['user', 'aap', 'movie', '55', 'return', 'feageed', 'indicator', '15', '\n",
      "eb6      |              |trade', 'year', 'awesome']                                              \n",
      "30464f4d4|True          |['user', 'id', 'afraid', 'short', 'amzn', 'looking', 'like', 'nearmonopo\n",
      "31a      |              |ly', 'ebooks', 'infrastructureasaservice']                              \n",
      "9e7926685|True          |['mnta', '1200']                                                        \n",
      "732      |              |                                                                        \n",
      "473e64d5b|True          |['oi', '2137']                                                          \n",
      "792      |              |                                                                        \n",
      "PandasDataFrame\n",
      "word:str|sentiment:bool|ct:long\n",
      "--------+--------------+-------\n",
      "aap     |True          |513    \n",
      "user    |True          |441    \n",
      "aap     |False         |407    \n",
      "short   |False         |362    \n",
      "today   |True          |253    \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# schema: id:str,sentiment:bool,words:[str]\n",
    "def tokenize(df:Iterable[Dict[str,Any]]) -> Iterable[List[Any]]:\n",
    "    lem=WordNetLemmatizer()\n",
    "    stop=set(stopwords.words('english'))\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for row in df:\n",
    "        words = row[\"text\"].lower().translate(translator).split(\" \")\n",
    "        words = [lem.lemmatize(w) for w in words if w not in stop and w!=\"\"]\n",
    "        yield [row[\"id\"], row[\"sentiment\"], words]\n",
    "        \n",
    "# schema: word:str, sentiment:bool\n",
    "def to_single(df:Iterable[Dict[str,Any]]) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        sentiment = row[\"sentiment\"]\n",
    "        for w in row[\"words\"]:\n",
    "            yield [w,sentiment]\n",
    "\n",
    "with fa.engine_context():\n",
    "    df = fa.load(\"../../../data/stock_sentiment_data.csv\", header=True)\n",
    "    tk = fa.transform(fa.transform(df,preprocess), tokenize)\n",
    "    fa.show(tk, 5)\n",
    "    words = fa.transform(tk, to_single)\n",
    "    fa.show(fa.raw_sql(\"word, sentiment, COUNT(*) AS ct FROM\",words,\"GROUP BY word, sentiment ORDER BY ct DESC LIMIT 10\"), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Detection\n",
    "\n",
    "Entity linking can generate very powerful features, so we want to do it for each sentence. Spacy is a popular package to use for this.\n",
    "\n",
    "Entity linking can be very expensive, so making it run distributedly is how we deal with large dataset. In Fugue, it helps you separate the concerns.\n",
    "\n",
    "* Transformer is to handle a partition of data on a single machine, so scalabity and throughput is not the concern of a transformer\n",
    "* How to run a created transformer is associated with scalability and throughput concerns. But again, Fugue is very abstract, you can just tell the system I want to apply the transformation and let the system to optimize the distribution. And actually you can have full control of the distribution, but here we don't go into too much details, let's just focus on *WHAT* instead of *HOW*\n",
    "\n",
    "Another thing to point out is that Fugue avoids the semantic of `map` and only uses `mapPartitions`. `entity_linking` in the following code is a perfect example to demonstrate why. `spacy.load` is expensive, but it's called only once and it can be used for all items in the partition. For the cases you don't have expensive initialization, this approach is neither more complicated nor slower than `map`. So there is no good reason to directly use `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "id:str    |sentiment:bool|entities:str                                                         \n",
      "----------+--------------+---------------------------------------------------------------------\n",
      "4102bd2ab9|True          |{\"pnk cpw bpz\": \"PRODUCT\", \"1\": \"CARDINAL\", \"2\": \"CARDINAL\"}         \n",
      "86        |              |                                                                     \n",
      "c0b462af2e|True          |{\"aap movie\": \"ORG\", \"55%\": \"PERCENT\", \"fea/geed\": \"ORG\", \"15\": \"CARD\n",
      "b6        |              |INAL\", \"the year\": \"DATE\"}                                           \n",
      "30464f4d43|True          |{\"ebooks\": \"ORG\"}                                                    \n",
      "1a        |              |                                                                     \n",
      "9e79266857|True          |{\"12.00\": \"CARDINAL\"}                                                \n",
      "32        |              |                                                                     \n",
      "473e64d5b7|True          |{\"21.37\": \"CARDINAL\"}                                                \n",
      "92        |              |                                                                     \n",
      "PandasDataFrame\n",
      "name:str|label:str|sentiment:bool|ct:long\n",
      "--------+---------+--------------+-------\n",
      "aap     |ORG      |True          |9      \n",
      "today   |DATE     |True          |6      \n",
      "bac     |ORG      |True          |4      \n",
      "goog    |ORG      |True          |4      \n",
      "aap     |ORG      |False         |3      \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "# schema: id:str,sentiment:bool,entities:str\n",
    "def entity_linking(df:Iterable[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    for row in df:\n",
    "        doc = nlp(row[\"text\"])\n",
    "        row[\"entities\"] = json.dumps({str(ent).lower():str(ent.label_) for ent in doc.ents})\n",
    "        yield row\n",
    "        \n",
    "\n",
    "# schema: name:str,label:str,sentiment:bool\n",
    "def to_single_entities(df:Iterable[Dict[str,Any]]) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        for k,v in json.loads(row[\"entities\"]).items():\n",
    "            yield [k,v,row[\"sentiment\"]]\n",
    "\n",
    "with fa.engine_context():\n",
    "    df = fa.load(\"../../../data/stock_sentiment_data.csv\", header=True)\n",
    "    df = fa.raw_sql(\"* FROM\",df,\" LIMIT 100\")\n",
    "    pre = fa.transform(fa.transform(df,preprocess),entity_linking)\n",
    "    fa.show(pre, 5)\n",
    "    entities = fa.transform(pre, to_single_entities)\n",
    "    fa.show(fa.raw_sql(\"name, label, sentiment, COUNT(*) AS ct FROM\",entities,\"GROUP BY name, label, sentiment ORDER BY ct DESC LIMIT 10\"), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring It to Spark!\n",
    "\n",
    "Now it seems both tokenization and entity detection are working well on small data locally. We are going to combine them together. To make it truly scalable, we will use Spark as the execution engine.\n",
    "\n",
    "Pay attention to a few things:\n",
    "\n",
    "* This logic can run on any execution engine, you may create an end to end test on small data using the Pandas-based `NativeExecutionEngine`\n",
    "* For the transformers it uses, they have no dependency on Fugue.\n",
    "\n",
    "*This step may be slow on binder, if possible, try it with larger data on a real Spark cluster*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "id:str |sentiment:bool|words:[str]                   |entities:str                    \n",
      "-------+--------------+------------------------------+--------------------------------\n",
      "4102bd2|True          |['kicker', 'watchlist', 'xide'|{\"pnk cpw bpz\": \"PRODUCT\", \"1\": \n",
      "ab986  |              |, 'tit', 'soq', 'pnk', 'cpw', |\"CARDINAL\", \"2\": \"CARDINAL\"}    \n",
      "       |              |'bpz', 'aj', 'trade', 'method'|                                \n",
      "       |              |, '1', 'method', '2', 'see', '|                                \n",
      "       |              |prev', 'post']                |                                \n",
      "c0b462a|True          |['user', 'aap', 'movie', '55',|{\"aap movie\": \"ORG\", \"55%\": \"PER\n",
      "f2eb6  |              | 'return', 'feageed', 'indicat|CENT\", \"fea/geed\": \"ORG\", \"15\": \n",
      "       |              |or', '15', 'trade', 'year', 'a|\"CARDINAL\", \"the year\": \"DATE\"} \n",
      "       |              |wesome']                      |                                \n",
      "30464f4|True          |['user', 'id', 'afraid', 'shor|{\"ebooks\": \"ORG\"}               \n",
      "d431a  |              |t', 'amzn', 'looking', 'like',|                                \n",
      "       |              | 'nearmonopoly', 'ebooks', 'in|                                \n",
      "       |              |frastructureasaservice']      |                                \n",
      "9e79266|True          |['mnta', '1200']              |{\"12.00\": \"CARDINAL\"}           \n",
      "85732  |              |                              |                                \n",
      "473e64d|True          |['oi', '2137']                |{\"21.37\": \"CARDINAL\"}           \n",
      "5b792  |              |                              |                                \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "with fa.engine_context(spark):\n",
    "    df = fa.load(\"../../../data/stock_sentiment_data.csv\", escape='\"', header=True)\n",
    "    pre = fa.transform(df,preprocess)\n",
    "    tokens = fa.transform(pre,tokenize)\n",
    "    entities = fa.transform(pre, entity_linking)\n",
    "    result = fa.inner_join(tokens,entities)\n",
    "    fa.show(result,5)\n",
    "    fa.save(result, \"/tmp/stock_sentiment.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, let's show an alternative way to describe the end to end logic -- using Fugue SQL. It also adds the steps to print some stats from tokens and entities. In this example, tokens and entities will also be auto persisted because they are also used for multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "id:str|sentiment:bool|text:str           |words:[str]        |entities:str           \n",
      "------+--------------+-------------------+-------------------+-----------------------\n",
      "4102bd|True          |Kickers on my watch|['kicker', 'watchli|{\"pnk cpw bpz\": \"PRODUC\n",
      "2ab986|              |list XIDE TIT SOQ P|st', 'xide', 'tit',|T\", \"1\": \"CARDINAL\", \"2\n",
      "      |              |NK CPW BPZ AJ  trad| 'soq', 'pnk', 'cpw|\": \"CARDINAL\"}         \n",
      "      |              |e method 1 or metho|', 'bpz', 'aj', 'tr|                       \n",
      "      |              |d 2, see prev posts|ade', 'method', '1'|                       \n",
      "      |              |                   |, 'method', '2', 's|                       \n",
      "      |              |                   |ee', 'prev', 'post'|                       \n",
      "      |              |                   |]                  |                       \n",
      "c0b462|True          |user: AAP MOVIE. 55|['user', 'aap', 'mo|{\"aap movie\": \"ORG\", \"5\n",
      "af2eb6|              |% return for the FE|vie', '55', 'return|5%\": \"PERCENT\", \"fea/ge\n",
      "      |              |A/GEED indicator ju|', 'feageed', 'indi|ed\": \"ORG\", \"15\": \"CARD\n",
      "      |              |st 15 trades for th|cator', '15', 'trad|INAL\", \"the year\": \"DAT\n",
      "      |              |e year.  AWESOME.  |e', 'year', 'awesom|E\"}                    \n",
      "      |              |                   |e']                |                       \n",
      "30464f|True          |user I'd be afraid |['user', 'id', 'afr|{\"ebooks\": \"ORG\"}      \n",
      "4d431a|              |to short AMZN - the|aid', 'short', 'amz|                       \n",
      "      |              |y are looking like |n', 'looking', 'lik|                       \n",
      "      |              |a near-monopoly in |e', 'nearmonopoly',|                       \n",
      "      |              |eBooks and infrastr| 'ebooks', 'infrast|                       \n",
      "      |              |ucture-as-a-service|ructureasaservice']|                       \n",
      "9e7926|True          |MNTA Over 12.00    |['mnta', '1200']   |{\"12.00\": \"CARDINAL\"}  \n",
      "685732|              |                   |                   |                       \n",
      "473e64|True          |OI  Over 21.37     |['oi', '2137']     |{\"21.37\": \"CARDINAL\"}  \n",
      "d5b792|              |                   |                   |                       \n",
      "tokens\n",
      "SparkDataFrame\n",
      "word:str|sentiment:bool|ct:long\n",
      "--------+--------------+-------\n",
      "aap     |True          |513    \n",
      "user    |True          |441    \n",
      "aap     |False         |407    \n",
      "short   |False         |362    \n",
      "today   |True          |253    \n",
      "volume  |True          |241    \n",
      "stock   |True          |241    \n",
      "day     |True          |223    \n",
      "long    |True          |210    \n",
      "user    |False         |202    \n",
      "entities\n",
      "SparkDataFrame\n",
      "name:str                                                           |label:str|sentiment:bool|ct:long\n",
      "-------------------------------------------------------------------+---------+--------------+-------\n",
      "aap                                                                |ORG      |True          |429    \n",
      "aap                                                                |ORG      |False         |351    \n",
      "today                                                              |DATE     |True          |234    \n",
      "bac                                                                |ORG      |True          |110    \n",
      "goog                                                               |ORG      |True          |91     \n",
      "today                                                              |DATE     |False         |78     \n",
      "goog                                                               |ORG      |False         |55     \n",
      "tomorrow                                                           |DATE     |True          |45     \n",
      "yesterday                                                          |DATE     |True          |40     \n",
      "one                                                                |CARDINAL |True          |39     \n"
     ]
    }
   ],
   "source": [
    "%%fsql spark\n",
    "\n",
    "LOAD \"../../../data/stock_sentiment_data.csv\"(header=true,escape='\"')\n",
    "df = TRANSFORM USING preprocess\n",
    "tokens = TRANSFORM df USING tokenize\n",
    "entities = TRANSFORM df USING entity_linking\n",
    "result =\n",
    "    SELECT df.*, words, entities\n",
    "    FROM df \n",
    "    INNER JOIN tokens ON df.id = tokens.id\n",
    "    INNER JOIN entities ON df.id = entities.id\n",
    "PRINT 5 ROWS\n",
    "SAVE OVERWRITE \"/tmp/stock_sentiment.parquet\"\n",
    "\n",
    "\n",
    "SELECT word, sentiment, COUNT(*) AS ct \n",
    "    FROM (TRANSFORM tokens USING to_single)\n",
    "    GROUP BY word, sentiment \n",
    "    ORDER BY ct DESC LIMIT 10\n",
    "PRINT TITLE \"tokens\"\n",
    "\n",
    "SELECT name, label, sentiment, COUNT(*) AS ct\n",
    "    FROM (TRANSFORM entities USING to_single_entities)\n",
    "    GROUP BY name, label, sentiment\n",
    "    ORDER BY ct DESC LIMIT 10\n",
    "PRINT TITLE \"entities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fugue is not only a framework, it's also a way of thinking -- you should keep your code as native as possible and it should be less coupled with any particular computing framework including Fugue itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
