{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Distributed Compute\n",
    "\n",
    "pandas is great for small datasets but unfortunately does not scale large datasets well. The primary reason is that pandas is single core and does not take advantage of all available computing resources. A lot of operations also generate [intermediate copies](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets) of data, utilizing more memory than necessary. To effectively handle data with pandas, users preferably need to have [5 to 10 times](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) as much RAM as the dataset.\n",
    "\n",
    "[Spark](https://spark.apache.org/) and [Dask](https://dask.org/) allow us to split computing jobs across multiple machines. They also can handle datasets that don’t fit into memory by spilling data over to the disk in some cases. But ultimately, moving to Spark or Dask still requires significant code changes to port existing pandas code. Added to changing code, there is also a lot of knowledge required to use these frameworks effectively.\n",
    "\n",
    "**Fugue is a framework that is designed to unify the interface between pandas, Spark, and Dask, allowing one codebase to be used across all three engines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fugue `transform()`\n",
    "\n",
    "The simplest way Fugue can be used to scale pandas based code to Spark or Dask is with the `transform()` function. In the example below, we’ll train a model using scikit-learn and pandas and then perform the inference parallelized on top of the Spark execution engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate jupyter: Running cells with 'Python 3.8.9 64-bit' requires jupyter and notebook package.\n",
      "Run the following command to install 'jupyter and notebook' into the Python environment. \n",
      "Command: 'python -m pip install jupyter notebook -U\n",
      "or\n",
      "conda install jupyter notebook -U'\n",
      "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we then wrap it in a `predict()` function. This function is still written in pandas. We can easily test it on the `input_df` that we create. Wrapping it will allow us to bring it to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_1  x_2  predicted\n",
       "0    3    3       12.0\n",
       "1    4    3       13.0\n",
       "2    6    6       21.0\n",
       "3    6    6       21.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n",
    "    return df.assign(predicted=model.predict(df))\n",
    "\n",
    "input_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n",
    "\n",
    "# test the function\n",
    "predict(input_df.copy(), reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we bring it to Spark using Fugue's `transform()` function. This takes in a DataFrame and applies a function to it using either of the pandas, Spark, or Dask engines. The `transform()` inputs will be explained later, but for now, notice that we did not make modifications to the pandas-based `predict()` function in order to use it on Spark. This function can now scale to big datasets through the Spark or Dask execution engines.\n",
    "\n",
    "Even if there is no Spark cluster available, the `SparkExecutionEngine` will start a local Spark instance and parallelize the jobs with all cores of the machine. Note the importing `fugue_spark` registers the `\"spark\"` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---+---+---------+\n",
      "|x_1|x_2|predicted|\n",
      "+---+---+---------+\n",
      "|  3|  3|     12.0|\n",
      "|  4|  3|     13.0|\n",
      "|  6|  6|     21.0|\n",
      "|  6|  6|     21.0|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from fugue import transform\n",
    "import fugue_spark\n",
    "\n",
    "result = transform(\n",
    "    input_df,\n",
    "    predict,\n",
    "    schema=\"*,predicted:double\",\n",
    "    params=dict(model=reg),\n",
    "    engine=\"spark\"\n",
    ")\n",
    "print(type(result))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform()` function takes in the following arguments:\n",
    "\n",
    "* df     - input DataFrame (can be a pandas, Spark, or Dask DataFrame)\n",
    "* using  - a Python function with valid type annotations\n",
    "* schema - output schema of the operation\n",
    "* params - a dictionary of parameters to pass in the function\n",
    "* engine - the execution engine to run the operation on pandas, Spark, or Dask \n",
    "\n",
    "Because we supplied `\"spark\"` as the engine, the `predict()` function will be applied on `input_df` on top of the Spark ExecutionEngine. Fugue will handle the conversion from a pandas DataFrame to a Spark DataFrame. Similarly, a Spark DataFrame can be passed to the `transform()` call. Supplying no engine uses the pandas-based `NativeExecutionEngine`. Fugue also has a `DaskExecutionEngine` available.\n",
    "\n",
    "Explicit `schema` is a hard requirement in distributed computing frameworks, so we need to supply the output `schema` of the operation. When compared to the Spark equivalent (see below), this is a much simpler interface to handle the `schema`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Fugue's `transform()` function can scale pandas-written code to Spark or Dask, without altering the existing functions. In the next section, we’ll take a deeper look at the `transform()` function. While we used pandas here, we’ll also show that native Python functions can also be used across the different execution engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Spark Equivalent of `transform()`\n",
    "\n",
    "Below is an example of how the `predict()` function would be brought to Spark without the `transform()` function. This implementation uses the Spark’s `mapInPandas()` method available in Spark 3.0. Note how the `schema` has to be handled inside the `run_predict` function. This is the `schema` requirement we mentioned earlier that Fugue provides a simpler interface for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "|x_1|x_2|predicted|\n",
      "+---+---+---------+\n",
      "|  3|  3|     12.0|\n",
      "|  4|  3|     13.0|\n",
      "|  6|  6|     21.0|\n",
      "|  6|  6|     21.0|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Any, Union\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def predict_wrapper(dfs: Iterator[pd.DataFrame], model):\n",
    "    for df in dfs:\n",
    "        yield predict(df, model)\n",
    "\n",
    "def run_predict(input_df: Union[DataFrame, pd.DataFrame], model):\n",
    "    # conversion\n",
    "    if isinstance(input_df, pd.DataFrame):\n",
    "        sdf = spark_session.createDataFrame(input_df.copy())\n",
    "    else:\n",
    "        sdf = input_df.copy()\n",
    "\n",
    "    schema = StructType(list(sdf.schema.fields))\n",
    "    schema.add(StructField(\"predicted\", DoubleType()))\n",
    "    return sdf.mapInPandas(lambda dfs: predict_wrapper(dfs, model), \n",
    "                           schema=schema)\n",
    "\n",
    "result = run_predict(input_df.copy(), reg)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s very easy to see why it becomes very difficult to bring a pandas codebase to Spark with this approach. We had to define two additional functions in the `predict_wrapper()` and the `run_predict()` to bring it to Spark. If this had to be done for tens of functions, it could easily fill the codebase with boilerplate code, making it hard to focus on the logic."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "f7f9294720e464cd08733c6cd5cfe1a4599977fa03668bc63f2dfd97f1a61807"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
