{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When data is small enough to fit on a laptop, it's simple for data practitioners to iterate on data projects. Most commonly, data practitioners use pandas and NumPy for their data analysis and feature engineering needs. These tools go well with scikit-learn to provide a stack capable of handling the end-to-end machine learning pipeline. This works great until data becomes too big to fit on a single machine. At this point, data practitioners need to move to distributed compute frameworks such as Spark and Dask to scale their solutions out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilizing Distributed Compute\n",
    "\n",
    "pandas is great for small datasets, but unfortunately does not scale well large datasets. The primary reason is that pandas is single core, and does not take advantage of all available compute resources. A lot of operations also generate [intermediate copies](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets) of data, utilizing more memory than necessary. To effectively handle data with pandas, users preferably need to have [5x to 10x times](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) as much RAM as the size of the dataset.\n",
    "\n",
    "This leads us to frameworks such as Spark and Dask. These frameworks allow us to split compute jobs across multiple machines. They also can handle datasets that don’t fit into memory by spilling data over to disk in some cases. Compared to Spark, Dask is the easier transition from pandas because it is built on top of the pandas DataFrames which means that there is strong parity between their APIs. But ultimately, moving to Spark or Dask still requires code changes to port pandas code. Added to changing code, there is also a big shift in mindset needed to fully utilize the distributed compute engines. \n",
    "\n",
    "**Fugue is a framework that is designed to unify the interface between pandas, Spark, and Dask, allowing one codebase to be used across all three engines.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fugue `transform`\n",
    "\n",
    "Fugue is an abstraction layer designed to provide a seamless transition from local compute to distributed compute. Using the abstraction layer allows users to take advantage of the Spark and Dask computation engines, while writing code in Python, pandas or SQL. This allows users to focus on the problems they are trying to solve, rather than learning a new framework for the job. This also provides other concrete benefits that we’ll see throughout this tutorial.\n",
    "\n",
    "The simplest way Fugue can be used to scale Pandas based code to Spark or Dask is the `transform` function. In the example below, we’ll train a model using scikit-learn and pandas, and then perform the `predict` step in parallel on top of the Spark execution engine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training our model, we then wrap it in a `predict` function. This function is still written in pandas. We can easily test it on the `input_df` that we create."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n",
    "    return df.assign(predicted=model.predict(df))\n",
    "\n",
    "input_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n",
    "\n",
    "# test the function\n",
    "predict(input_df.copy(), reg)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   x_1  x_2  predicted\n",
       "0    3    3       12.0\n",
       "1    4    3       13.0\n",
       "2    6    6       21.0\n",
       "3    6    6       21.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we bring it to Spark using Fugue in the next code snippets. Fugue has a function called transform that takes in a DataFrame and applies a function to it distributedly using Spark engine or Dask engine. We’ll explain the inputs that go into this function in a bit (but they should be intuitive). The important thing to notice is that we did not make modifications to the pandas-based `predict` function in order to use it on Spark. This function can now scale to big datasets through the Spark execution engine.\n",
    "\n",
    "Even if there is no cluster available, the `SparkExecutionEngine` will start a local Spark instance and parallelize the jobs with all cores of the machine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# create Spark session for next cells\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from fugue import transform\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "result = transform(\n",
    "    input_df,\n",
    "    predict,\n",
    "    schema=\"*,predicted:double\",\n",
    "    params=dict(model=reg),\n",
    "    engine=SparkExecutionEngine(spark_session)\n",
    ")\n",
    "result.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+---------+\n",
      "|x_1|x_2|predicted|\n",
      "+---+---+---------+\n",
      "|  3|  3|     12.0|\n",
      "|  4|  3|     13.0|\n",
      "|  6|  6|     21.0|\n",
      "|  6|  6|     21.0|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first two arguments of the `transform` function are the DataFrame to operate on and the function to use. The `input_df` can either be a pandas DataFrame or a Spark DataFrame. The engine then dictates what execution engine to use for the computation. Because we supplied a pandas DataFrame with the `SparkExecutionEngine`, that DataFrame was converted to be used in Spark. The output of this function is a Spark DataFrame because the engine used was the `SparkExecutionEngine`. Supplying no engine uses the pandas-based `NativeExecutionEngine`. Fugue also has a `DaskExecutionEngine` available.\n",
    "\n",
    "The other two arguments are the `schema` and `params`. Explicit `schema` is a hard requirement in distributed computing frameworks, so we need to supply the output `schema` of the operation. When compared to the Spark equivalent (seen below), this is a much simpler interface to handle the `schema`. Lastly, `params` is a dictionary that contains other inputs into the function. In this case, we passed in the regression model to be used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "With that, we have shown the use-case of Fugue in scaling pandas-written code to Spark. It can be done in very few lines of code, without altering the existing code base. In the next section, we’ll see other features Fugue has to offer, and the other ways it simplifies using distributed compute. By using the `transform` function, we allowed the `predict` function to be applicable to both pandas and Spark. We’ll apply this same concept to entire workflows in the next section.\n",
    "\n",
    "While we used pandas here, we’ll also show that native Python functions can also be used across the different execution engines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Optional] Spark Equivalent of `transform`\n",
    "\n",
    "If you are wondering how `transform` compares to implementing the same logic in Spark, below is an example of how the pandas function would be implemented in Spark if you did it yourself. This implementation uses the Spark’s `mapInPandas` method available in Spark 3.0. Note how the `schema` has to be handled inside the `run_predict` function. This is the `schema` requirement we mentioned earlier that Fugue provides a simpler interface for."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from typing import Iterator, Any, Union\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "def predict_wrapper(dfs: Iterator[pd.DataFrame], model):\n",
    "    for df in dfs:\n",
    "        yield predict(df, model)\n",
    "\n",
    "def run_predict(input_df: Union[DataFrame, pd.DataFrame], model):\n",
    "    # conversion\n",
    "    if isinstance(input_df, pd.DataFrame):\n",
    "        sdf = spark_session.createDataFrame(input_df.copy())\n",
    "    else:\n",
    "        sdf = input_df.copy()\n",
    "\n",
    "    schema = StructType(list(sdf.schema.fields))\n",
    "    schema.add(StructField(\"predicted\", DoubleType()))\n",
    "    return sdf.mapInPandas(lambda dfs: predict_wrapper(dfs, model), \n",
    "                           schema=schema)\n",
    "\n",
    "result = run_predict(input_df.copy(), reg)\n",
    "result.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+---------+\n",
      "|x_1|x_2|predicted|\n",
      "+---+---+---------+\n",
      "|  3|  3|     12.0|\n",
      "|  4|  3|     13.0|\n",
      "|  6|  6|     21.0|\n",
      "|  6|  6|     21.0|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It’s very easy to see why it becomes very difficult to bring a pandas codebase to Spark with this approach. We had to define two additional functions in the `predict_wrapper` and the `run_predict` to bring it to Spark. If this had to be done for tens of functions, it could easily fill the codebase with boilerplate code, making it hard to focus on the logic."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "f7f9294720e464cd08733c6cd5cfe1a4599977fa03668bc63f2dfd97f1a61807"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}