{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "The FugueWorkflow object creates a Directed Acyclic Graph (DAG) where the nodes are DataFrames that are connected by extensions. Extensions are code that creates/modifies/outputs DataFrames. The `Transformer` we have been using is an example of an extension. In this section, we'll cover the other types of extensions: `Creator`, `Processor`, `Outputter`, and `CoTransformer`. For all extensions, schema has to be defined. Below are the types of extensions.\n",
    "\n",
    "![extensions](../../images/extensions.svg)\n",
    "\n",
    "`OutputTransformer` and `OutputcCotransformer` will be covered in the Deep Dive section. \n",
    "\n",
    "We have actually already seen some built-in extensions that come with Fugue. For example, `load` is a `Creator` and `save` is an `Outputter`. There is a difference between `Driver side` and `Worker side` extensions. This will be covered at the end of this section. For now, we'll just see the syntax and use case for each extension.\n",
    "\n",
    "## Creator\n",
    "\n",
    "A `Creator` is an extension that takes no DataFrame as input but returns a DataFrame as output. It is used to generate DataFrames. Custom `Creators` can be used to load data from different sources (think AWS S3 or from a Database using pyodbc). Similar to the `Transformer` in the previous section, `Creators` can be defined with the schema hint comment or with the `@creator` decorator. `pd.DataFrame` is a special output type that does not require schema. For other output type hints, the schema is unknown so it needs to be defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "1    |2    \n",
      "2    |3    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# no need to define schema for pd.DataFrame\n",
    "def create_data() -> pd.DataFrame:\n",
    "    df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4]})\n",
    "    return df\n",
    "\n",
    "# schema: a:int, b:int\n",
    "def create_data2() -> List[Dict[str, Any]]:\n",
    "    df = [{'a':1, 'b':2}, {'a':2, 'b':3}]\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "     df = dag.create(create_data)\n",
    "     df2 = dag.create(create_data2)\n",
    "     df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processor\n",
    "\n",
    "A `Processor` is an extension that takes in one or more DataFrames and then outputs one DataFrame. Similar to the `Creator`, schema does not need to be specified for pd.DataFrame because it is already known. Schema needs to be specified for other output types. The `Processor` can be defined using the schema hint or the `@processor` decorator with the schema passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "1    |2    \n",
      "2    |3    \n",
      "1    |2    \n",
      "2    |3    \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def concat(df1:pd.DataFrame, df2:pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df1,df2]).reset_index(drop=True)\n",
    "\n",
    "# schema: a:int, b:int\n",
    "def fillna(df:List[Dict[str,Any]], n=0) -> List[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        if row['a'] is None:\n",
    "            row['a'] = n\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "     df = dag.create(create_data2)    # create_data2 is from earlier\n",
    "     df2 = dag.create(create_data2)\n",
    "     df3 = dag.process(df , df2, using=concat)\n",
    "     df3 = dag.process(df3, using=fillna, params={'n': 10})\n",
    "     df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of a `fillna` processor, but this is a common operation so there is actually a built-in operation for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "1    |2    \n",
      "2    |3    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with FugueWorkflow() as dag:\n",
    "     df = dag.create(create_data2)\n",
    "     df = df.fillna(10, subset=[\"a\"])\n",
    "     df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputter\n",
    "\n",
    "`Outputters` are extensions with one or more DataFrames as an input and no DataFrames at the output. We mentioned earlier that `save()` was an example of an `Outputter`. `show()` is actually another example too. Outputters can be used to write to S3 or upload to database. The output type of Outputters must be `None`. No schema is needed since it is a terminal operation. There is an `@outputter` decorator, but it doesn't do much because the return type is already `None`. Outputters are also used for plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "def head(df:List[List[Any]], n=1) -> None:\n",
    "    for i in range(n):\n",
    "        print(df[i])\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.create(create_data2)\n",
    "    dag.output(df, using=head, params={'n': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "`Transformer` is the most widely used extension. We have covered this in previous sections but more formally, a `Transformer` is an extension that takes in a DataFrame and returns a DataFrame. Schema needs to be explicit. Most logic will go into `Transformers`. Below is an example to create a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int|c:int\n",
      "-----+-----+-----\n",
      "1    |2    |3    \n",
      "2    |3    |5    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#schema: *, c:int\n",
    "def sum_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['c'] = df['a'] + df['b']\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.create(create_data2).fillna(10)\n",
    "    df = df.transform(using=sum_cols)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoTransformer\n",
    "\n",
    "The `CoTransformer` is very similar to the `Transformer`, except that it is intended to execute on multiple DataFrames that are partitioned in the same way. In order to use a `CoTransformer`, the `zip()` method has to be used first to join them by their common keys. There is also a `@cotransformer` decorator that can be used to define the `CoTransformer`, but it will still be invoked by the `zip-transform` syntax.\n",
    "\n",
    "In the example below, we will do a merge as-of operation on different groups of data. In order to align the data with events as they get distributed across the cluster, we will partition them in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>year</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group  year  value\n",
       "0     A  2014      1\n",
       "1     A  2016      2\n",
       "2     B  2014      1\n",
       "3     B  2018      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'group': ([\"A\"] * 5 + [\"B\"] * 5),\n",
    "                     'year': [2015,2016,2017,2018,2019] * 2})\n",
    "\n",
    "events = pd.DataFrame({'group': [\"A\", \"A\", \"B\", \"B\"],\n",
    "                       'year': [2014, 2016, 2014, 2018],\n",
    "                       \"value\": [1, 2, 1, 2]})\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas `merge_asof` function requires that the `on` column is sorted. To do this, we apply a `partition` strategy on Fugue by group and presort by the year. By the time it arrives in the `CoTransformer`, the dataframes are sorted and grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "group:str|year:int|value:int\n",
      "---------+--------+---------\n",
      "A        |2015    |1        \n",
      "A        |2016    |2        \n",
      "A        |2017    |2        \n",
      "A        |2018    |2        \n",
      "A        |2019    |2        \n",
      "B        |2015    |1        \n",
      "B        |2016    |1        \n",
      "B        |2017    |1        \n",
      "B        |2018    |2        \n",
      "B        |2019    |2        \n"
     ]
    }
   ],
   "source": [
    "# schema: group:str,year:int,value:int\n",
    "def merge_asof(data:pd.DataFrame, events:pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge_asof(data, events, on=\"year\", by=\"group\")\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    data = dag.df(data)\n",
    "    events = dag.df(events)\n",
    "\n",
    "    data.zip(events, partition={\"by\": \"group\", \"presort\": \"year\"}).transform(merge_asof).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the important part to note is each group uses the pandas `merge_asof` independently. This function is very flexible, allowing users to specify forward and backward merges along with a tolerance. This is tricky to implement well in Spark, but the `CoTransformer` lets us do it easily.\n",
    "\n",
    "This operation was partitioned by the column `group` before the `cotransform` was applied. This was done through the `zip` command. `CoTransform` is a more advanced operation that may take some experience to get used to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section we have gone over the building blocks of a `FugueWorkflow` in Fugue extensions. Extensions are abstractions for the different kinds of operations done on DataFrames. Fugue has the most common extensions built-in, but it will be very common for users to make their own extensions (especially `Transformers`) to work with DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Driver Extensions vs Worker Extensions\n",
    "\n",
    "For those less familiar with distributed systems, the work is spread across multiple workers, often referred to as a cluster. The driver is the machine that orchestrates the work done by the workers. For Fugue extensions, `Transformer` and `CoTransformer` are extensions that happen on the worker. Actions that happen on the worker-level are already agnostic to the `ExecutionEngine`.\n",
    "\n",
    "On the other hand, driver side extensions are `ExecutionEngine`-aware. This means that these extensions can use code written with Spark or Dask specifically. All we need to do is pass a first argument with the `ExecutionEngine` type annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "a:int\n",
      "-----\n",
      "2    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import ExecutionEngine, DataFrame\n",
    "from fugue_spark import SparkExecutionEngine, SparkDataFrame\n",
    "\n",
    "# pay attention to the input and output annotations, they are both general DataFrame\n",
    "def create(e:ExecutionEngine, n=1) -> DataFrame:\n",
    "    assert isinstance(e,SparkExecutionEngine) # this extension only works with SparkExecutionEngine\n",
    "    sdf= e.spark_session.createDataFrame([[n]], schema=\"a:int\")  # this is how you get spark session\n",
    "    return SparkDataFrame(sdf) # you must wrap as Fugue SparkDataFrame to return\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    dag.create(create, params={\"n\":2}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we lose cross-platform execution, but this can be used when users need to write Spark-specific code. `createDataFrame` is a Spark-specific method. This approach is Fugue's way of exposing the underlying ExecutionEngine if users want to use it. `creator`, `processor`, and `outputter` are all ExecutionEngine-aware. For users who are not as familiar with Spark, the recommendation is to write ExecutionEngine-agnostic code. That offers the most benefit of using Fugue because of the portability it provides.\n",
    "\n",
    "Something very tempting for beginner Fugue users is using something like `sklearn.MinMaxScaler`, which normalizes a column based on the minimum and maximum values. There is different behavior if your normalizing logic happens on the driver versus on the workers. On the workers, this happens locally without access to the global dataset. The min and max obtained for scaling happen on the partition level. On the other hand, using the Spark MinMaxScaler obtains the global min and max values for scaling.\n",
    "\n",
    "Think in terms of `map` operations and `aggregate` where `map` is row-wise and `aggregate` is column-wise. For `map` operations, the behavior of `Transformer` and `Processor` will be equivalent in most use cases. For `aggregate` operations, you can get different values depending on whether the execution is driver-side or worker-side."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
