{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extensions\n",
    "\n",
    "The FugueWorkflow object creates a Directed Acyclic Graph (DAG) where the nodes are DataFrames that are connected by extensions. Extensions are code that creates/modifies/outputs DataFrames. The `transformer` we have been using is an example of an extension. In this section, we'll cover the other types of extensions: `creator`, `processor`, `outputter`, and `cotransformer`. For all extensions, schema has to be defined. Below are the types of extensions.\n",
    "\n",
    "![extensions](../../images/extensions.svg)\n",
    "\n",
    "`outputtransformer` and `outputcotransformer` will be covered in the Deep Dive section. \n",
    "\n",
    "We have actually already seen some built-in extensions that come with Fugue. For example, `load` is a `creator` and `save` is an `outputter`. There is a difference between `Driver side` and `Worker side` extensions. This will be covered at the end of this section. For now, we'll just see the syntax and use case for each extension.\n",
    "\n",
    "## Creator\n",
    "\n",
    "A creator is an extension that takes no DataFrame as input, but returns a DataFrame as output. It is used to generate DataFrames. Custom creators can be used to load data from different sources (think AWS S3 or from a Database using pyodbc). Similar to the `transformer` in the previous section, `creators` can be defined with the schema hint comment, or with the `@creator` decorator. `pd.DataFrame` is a special output type that does not require schema. For other output type hints, the schema is unknown so it needs to be defined.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def create_data() -> pd.DataFrame:\n",
    "    df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4]})\n",
    "    return df\n",
    "\n",
    "# schema: a:int, b:int\n",
    "def create_data2() -> List[Dict[str, Any]]:\n",
    "    df = [{'a':1, 'b':2}, {'a':2, 'b':3}]\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "     df = dag.create(create_data)\n",
    "     df2 = dag.create(create_data2)\n",
    "     df2.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IterableDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "1    |2    \n",
      "2    |3    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processor\n",
    "\n",
    "A `processor` is an extension that takes in one or more DataFrames, and then outputs one DataFrame. Similar to the `creator`, schema does not need to be specified for pd.DataFrame because it is already known. Schema needs to be specified for other output types. The processor can be defined using the schema hint, or the `@processor` decorator with the schema passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def concat(df1:pd.DataFrame, df2:pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df1,df2]).reset_index(drop=True)\n",
    "\n",
    "# schema: a:double, b:double\n",
    "def fillna(df:List[Dict[str,Any]], n=0) -> List[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        if row['a'] is None:\n",
    "            row['a'] = n\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "     df = dag.create(create_data2)    # create_data2 is from earlier\n",
    "     df2 = dag.create(create_data2)\n",
    "     df3 = dag.process(df , df2, using=concat)\n",
    "     df3 = dag.process(df3, using=fillna, params={'n': 10})\n",
    "     df3.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IterableDataFrame\n",
      "a:double|b:double\n",
      "--------+--------\n",
      "1.0     |2.0     \n",
      "2.0     |3.0     \n",
      "1.0     |2.0     \n",
      "2.0     |3.0     \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we show an example for of a `fillna` processor, but this is a common operation so there is actually a built-in operation for it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with FugueWorkflow() as dag:\n",
    "     df = dag.create(create_data2)\n",
    "     df = df.fillna(10, subset=[\"a\"])\n",
    "     df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "1    |2    \n",
      "2    |3    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outputter\n",
    "\n",
    "Outputters are extensions with one of more DataFrames as an input, and no DataFrames at the output. We mentioned earlier that `save` was an example of an outputter. `show` is actually another example. Outputters can be used to write to S3, or upload to database. The output type of Outputters must be `None`. No schema is needed since it is a terminal operation. There is an `@outputter` decorator, but it doesn't do much because the return type is already `None`. Outputters are also used for plotting functions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def head(df:List[List[Any]], n=1) -> None:\n",
    "    for i in range(n):\n",
    "        print(df[i])\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.create(create_data2)\n",
    "    dag.output(df, using=head, params={'n': 2})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2]\n",
      "[2, 3]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer\n",
    "\n",
    "Transformer is the most widely used extension. We have covered this in previous sections but more formally, a transformer is an extension that takes in a DataFrame and returns a DataFrame. Schema needs to be explicit. Most logic will go into transformers. Below is an example to create a new column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#schema: *, c:int\n",
    "def sum_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['c'] = df['a'] + df['b']\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.create(create_data2).fillna(10)\n",
    "    df = df.transform(using=sum_cols)\n",
    "    df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int|c:int\n",
      "-----+-----+-----\n",
      "1    |2    |3    \n",
      "2    |3    |5    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CoTransformer\n",
    "\n",
    "The `cotransformer` is very similar to the `transformer`, except that it is intended to execute on multiple DataFrames that are partitioned in the same way. The next section will cover CoTransformer, as it is coupled with distributed computing concepts that need to be introduced."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this section we have gone over the building blocks of a FugueWorkflow in Fugue extensions. Extensions are abstractions for the different kinds of operations done on DataFrames. Fugue has the most common extensions built-in, but it will be very common for users to make their own extensions (especially transformers) to work with DataFrames."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Optional] Driver Extensions vs Worker Extensions\n",
    "\n",
    "For those unfamiliar with distributed systems, the work is spread across multiple workers, often referred to as a cluster. The driver is the machine that orchestrates the work done by the workers. For Fugue extensions, `transformer` and `cotransformer` are extensions that happen on the worker. Actions that happen on the worker-level are already agnostic to the ExecutionEngine.\n",
    "\n",
    "On the other hand, driver side extensions are ExecutionEngine aware. This means that these extensions can use code written with Spark or Dask specifically. All we need to do is to pass a first argument with the ExecutionEngine type annotation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from fugue import ExecutionEngine, DataFrame\n",
    "from fugue_spark import SparkExecutionEngine, SparkDataFrame\n",
    "\n",
    "# pay attention to the input and output annotations, they are both general DataFrame\n",
    "def create(e:ExecutionEngine, n=1) -> DataFrame:\n",
    "    assert isinstance(e,SparkExecutionEngine) # this extension only works with SparkExecutionEngine\n",
    "    sdf= e.spark_session.createDataFrame([[n]], schema=\"a:int\")  # this is how you get spark session\n",
    "    return SparkDataFrame(sdf) # you must wrap as Fugue SparkDataFrame to return\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    dag.create(create, params={\"n\":2}).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the code above, we lose cross-platform execution, but this can be used when users need to write Spark specific code. `createDataFrame` is a Spark specific method. This approach is Fugue's way of exposing the underlying ExecutionEngine if users want to use it. `creator`, `processor` and `outputter` are ExecutionEngine aware. For users who are not as familair with Spark, the recommendation is to write ExecutionEngine-agnostic code. That gives the most benefit of using Fugue because of the portability it provides."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('fugue-tutorials': conda)",
   "metadata": {
    "interpreter": {
     "hash": "131b24c7e1bb8763ab2b04d5b6d98a68c7b3a823a2a57c5722935f7690890f70"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}