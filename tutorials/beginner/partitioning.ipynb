{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning\n",
    "\n",
    "Have questions? Chat with us on Github or Slack:\n",
    "\n",
    "[![Homepage](https://img.shields.io/badge/fugue-source--code-red?logo=github)](https://github.com/fugue-project/fugue)\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "In the last section, we discussed how to define schema for operations in Fugue. In this case, we look at partitions, an important concept for distributed computing.\n",
    "\n",
    "The `partition` argument allows us to control the partitioning scheme before the `transform()` operation is applied. Partitions dictate the physical grouping of data within a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Partitioning Example\n",
    "\n",
    "In the DataFrame below, we want to take the difference of the value per day. Because there are three different ids, we want to make sure that we don't get the difference across ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date id  value\n",
       "0  2021-01-01  A      3\n",
       "1  2021-01-02  A      4\n",
       "2  2021-01-03  A      2\n",
       "3  2021-01-01  B      1\n",
       "4  2021-01-02  B      2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.DataFrame({\"date\":[\"2021-01-01\", \"2021-01-02\", \"2021-01-03\"] * 3,\n",
    "                   \"id\": ([\"A\"]*3 + [\"B\"]*3 + [\"C\"]*3),\n",
    "                   \"value\": [3, 4, 2, 1, 2, 5, 3, 2, 3]})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function that takes in a `pd.DataFrame` and outputs a `pd.DataFrame`. This will allow us to bring the logic to Spark and Dask as we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['diff'] = df['value'].diff()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we use the function directly seen below, we notice that the first row of B has a `value` instead of a `NaN`. This is wrong since the function used the `value` from A to calculate the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date id  value  diff\n",
       "0  2021-01-01  A      3   NaN\n",
       "1  2021-01-02  A      4   1.0\n",
       "2  2021-01-03  A      2  -2.0\n",
       "3  2021-01-01  B      1  -1.0\n",
       "4  2021-01-02  B      2   1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue import transform\n",
    "transform(data.copy(), \n",
    "          diff, \n",
    "          schema=\"*, diff:int\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is solved by passing the partitions to Fugue's `transform()`. We now see the correct output of `NaN` for the first value of B seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date id  value  diff\n",
       "0  2021-01-01  A      3   NaN\n",
       "1  2021-01-02  A      4   1.0\n",
       "2  2021-01-03  A      2  -2.0\n",
       "3  2021-01-01  B      1   NaN\n",
       "4  2021-01-02  B      2   1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(data.copy(), \n",
    "          diff, \n",
    "          schema=\"*, diff:int\",\n",
    "          partition={\"by\": \"id\"}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Partitions\n",
    "\n",
    "What happens if we don't supply partitions when we call `transform()`? For Spark and Dask, there are default partitions that are used. For some operations, row-wise operations for example, the default partitions should work. But when your groups of data need to be processed together, then partitions should be specified as the grouping mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what partitions look like, we create a `count()` function that will just count the number of elements in a given partition. If we use it naively without specifying partitions, we will see that the data is not grouped properly. There are many partitions with just one item of data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/08 15:25:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  A|    2|\n",
      "|  A|    2|\n",
      "|  B|    2|\n",
      "|  C|    2|\n",
      "|  C|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def count(df: pd.DataFrame) -> List[Dict[str,Any]]:\n",
    "    return [{\"id\": df.iloc[0][\"id\"], \"count\": df.shape[0]}]\n",
    "\n",
    "transform(data.copy(),\n",
    "          count,\n",
    "          schema=\"id:str, count:int\",\n",
    "          engine=spark).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we specify the partitions by id, then each id will be grouped into the same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  B|    3|\n",
      "|  C|    3|\n",
      "|  A|    3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform(data.copy(),\n",
    "          count,\n",
    "          schema=\"id:str, count:int\",\n",
    "          engine=spark,\n",
    "          partition={\"by\":\"id\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presort\n",
    "\n",
    "Fugue's partition also takes in a `presort` that will sort the data before the `transform()` function is applied. For example, we can get the row with the maximum value for each id by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date id  value\n",
       "0  2021-01-02  A      4\n",
       "1  2021-01-03  B      5\n",
       "2  2021-01-01  C      3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_row(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.head(1)\n",
    "\n",
    "transform(data.copy(), \n",
    "          one_row,\n",
    "          schema=\"*\",\n",
    "          partition={\"by\":\"id\",\"presort\":\"value desc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the row with the minumum `value` can be taken by using `value asc` as the presort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition-specific Behavior\n",
    "\n",
    "Fugue also makes it possible to modify the logic that is applied for each partition of data. For example, we can create a function that has a different behavior for `id==A` and a different behavior for `id==B or id==C`. In the function below, the data with `id==A` will be clipped with a minimum of 0 and maximum of 4. The other groups will have a minimum of 1 and maximum of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    id = df.iloc[0][\"id\"]\n",
    "    if id == \"A\":\n",
    "        df = df.assign(value = df['value'].clip(0,4))\n",
    "    else:\n",
    "        df = df.assign(value = df['value'].clip(1,2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we call it with the `transform()` function, the values of rows with `id` of B or C will have a range of values 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+\n",
      "|      date| id|value|\n",
      "+----------+---+-----+\n",
      "|2021-01-01|  B|    1|\n",
      "|2021-01-02|  B|    2|\n",
      "|2021-01-03|  B|    2|\n",
      "|2021-01-01|  C|    2|\n",
      "|2021-01-02|  C|    2|\n",
      "|2021-01-03|  C|    2|\n",
      "|2021-01-01|  A|    3|\n",
      "|2021-01-02|  A|    4|\n",
      "|2021-01-03|  A|    2|\n",
      "+----------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform(data.copy(),\n",
    "          clip,\n",
    "          schema=\"*\",\n",
    "          partition={\"by\":\"id\"},\n",
    "          engine=spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Validation\n",
    "\n",
    "In order for the functions that we defined above to behave correctly, we required the data to be partitioned or sorted. To ensure that we have configured the data correctly before passing it to our functions, we can define a partition validation check that instructs Fugue to throw an error if partitioning or presorting have not been aptly applied. Note the comments above the function defined below which will be read and applied. Even if Fugue is not used, they serve as helpful comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  col2\n",
       "0    a     3\n",
       "1    a     2\n",
       "2    b     6\n",
       "3    b     5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "df = pd.DataFrame({\"col1\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"],\n",
    "                   \"col2\": [1,2,3,4,5,6]})\n",
    "\n",
    "# partitionby_has: col1\n",
    "# presort_is: col2 desc\n",
    "def top_two(df:List[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Function that returns the top 2 rows of an iterable.\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    while n < 2:\n",
    "        yield df[n]\n",
    "        n = n + 1\n",
    "\n",
    "transform(\n",
    "    df=df, \n",
    "    using=top_two, \n",
    "    schema=\"*\", \n",
    "    partition={\"by\":\"col1\", \"presort\": \"col2 desc\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code snippet exhibits a function operating on data that has been correctly partitioned and/or presorted. Now take a look at what happens if we don't apply these steps correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required presort key col2 is not in presort of PartitionSpec(num='0', by=['col1'], presort='')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    transform(\n",
    "        df=df, \n",
    "        using=top_two, \n",
    "        schema=\"*\", \n",
    "        partition={\"by\":\"col1\"}\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Strategies (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fugue also offers a number of other partitioning strategies that can be used aside from the column level strategy described above. See below for a brief summary of these strategies. Also, make sure to check out [partitioning](advanced/partition.ipynb) for further information.\n",
    "\n",
    "**Algo**\n",
    "* even - enforces an even number of items per partition\n",
    "* rand - randomly shuffles data\n",
    "* hash - uses hashing to partition the data (similar to Spark default)\n",
    "\n",
    "**Num**\n",
    "* A number of partitions can be supplied as the partitioning strategy. Note that this strategy will not work for the pandas-based engine.\n",
    "\n",
    "These strategies can also be used with `presort`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  col2\n",
       "0    a     1\n",
       "0    a     2\n",
       "0    a     3\n",
       "0    b     4\n",
       "0    b     5\n",
       "0    b     6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def no_op(df:List[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Function that returns the first row of an iterable.\n",
    "    \"\"\"\n",
    "    yield df[0]\n",
    "\n",
    "# by number\n",
    "transform(\n",
    "    df=df, \n",
    "    using=no_op, \n",
    "    schema=\"*\", \n",
    "    partition={\"num\":4}, \n",
    "    engine=\"dask\"\n",
    "    ).compute()\n",
    "\n",
    "# by algorithm\n",
    "transform(\n",
    "    df=df, \n",
    "    using=no_op, \n",
    "    schema=\"*\", \n",
    "    partition={\"algo\":\"even\"}, \n",
    "    engine=\"dask\"\n",
    "    ).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `transform()` runs on each partition of data. When using the pandas-based engine, the whole DataFrame is treated as one partition. For example, if you are normalizing a column of data, it will be applied on a per partition basis.\n",
    "\n",
    "If you need to perform an operation that requires global max/mean/min, then Fugue has a more advanced interface for that called `FugueWorkflow`, or you can use the native Spark/Dask operations after a `transform()` call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations Across Whole DataFrame\n",
    "\n",
    "There are some cases where we need to perform some operations on a whole dataset. Upto now, the `transform()` has taken an operation and applied it per partition. How can we do a min max scaling on Spark? In Fugue semantics, we call this as a `Processor`. Processors operate on the whole DataFrame on the Driver side, while `Transformers` are meant to run inside a worker and focus on local logic.\n",
    "\n",
    "### Using FugueSQL\n",
    "\n",
    "The simplest way is to use FugueSQL because it's inherently scale agnostic and will work for both Spark and Pandas (along with Dask). Below is a way to do the min-max scaling across a whole column. For practitioners that don't want to use SQL, the Python equivalent can be found below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  col2_scaled\n",
       "0    a          0.0\n",
       "1    a          0.2\n",
       "2    a          0.4\n",
       "3    b          0.6\n",
       "4    b          0.8\n",
       "5    b          1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue.api import fugue_sql\n",
    "\n",
    "df = pd.DataFrame({\"col1\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"],\n",
    "                   \"col2\": [1,2,3,4,5,6]})\n",
    "\n",
    "query = \"\"\"\n",
    "temp = SELECT MIN(col2) AS minval, MAX(col2) AS maxval\n",
    "         FROM df\n",
    "\n",
    "SELECT col1, (col2 - minval) / (maxval-minval) AS col2_scaled\n",
    "  FROM temp\n",
    " CROSS JOIN df\n",
    "\"\"\"\n",
    "\n",
    "fugue_sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to run this on Spark, all we have to do is change the engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|col1|col2_scaled|\n",
      "+----+-----------+\n",
      "|   a|        0.0|\n",
      "|   a|        0.2|\n",
      "|   a|        0.4|\n",
      "|   b|        0.6|\n",
      "|   b|        0.8|\n",
      "|   b|        1.0|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "\n",
    "fugue_sql(query, engine=spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Fugue API\n",
    "\n",
    "Fugue has a programmatic API to do such operations, but users tend to find it to be less intuitive. This is closer to Spark's syntax than Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  col2_scaled\n",
       "0    a          0.0\n",
       "1    a          0.2\n",
       "2    a          0.4\n",
       "3    b          0.6\n",
       "4    b          0.8\n",
       "5    b          1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue.column import col, lit, functions as f\n",
    "import fugue.api as fa\n",
    "from fugue import AnyDataFrame\n",
    "\n",
    "df = pd.DataFrame({\"col1\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"],\n",
    "                   \"col2\": [1,2,3,4,5,6]})\n",
    "\n",
    "def scale(df: AnyDataFrame):\n",
    "    minmax = fa.as_array(fa.select(df, f.min(col(\"col2\")).alias(\"minval\"), f.max(col(\"col2\")).alias(\"maxval\")))\n",
    "    return fa.select(\n",
    "        df,\n",
    "        col(\"col1\"),\n",
    "        ((col(\"col2\") - minmax[0][0])/ (minmax[0][1] - minmax[0][0])).alias(\"col2_scaled\"),\n",
    "    )\n",
    "\n",
    "scale(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to run on Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|col1|col2_scaled|\n",
      "+----+-----------+\n",
      "|   a|        0.0|\n",
      "|   a|        0.2|\n",
      "|   a|        0.4|\n",
      "|   b|        0.6|\n",
      "|   b|        0.8|\n",
      "|   b|        1.0|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(df)\n",
    "\n",
    "scale(sdf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of column functions, see [the documentation](https://fugue.readthedocs.io/en/latest/api/fugue.column.html#module-fugue.column.functions) but FugueSQL is more complete than the Python API for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this section we have shown the `partition-transform` semantics, which are equivalent to the Pandas `groupby-apply`. The difference is this scales to Spark, Dask, or Ray seamlessly because it dictates the logical and physical grouping of data in distributed settings.\n",
    "\n",
    "In the next section, we take a look at the ways to define the execution engine to bring our functions to Spark, Dask, or Ray."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
