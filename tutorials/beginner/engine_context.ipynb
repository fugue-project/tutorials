{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine Context\n",
    "\n",
    "Have questions? Chat with us on Github or Slack:\n",
    "\n",
    "[![Homepage](https://img.shields.io/badge/fugue-source--code-red?logo=github)](https://github.com/fugue-project/fugue)\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "So far we've used Fugue's `transform()` function to port Pandas code to Spark, Dask, and Ray without any rewrites. We also the Fugue API functions `save()` and `load()` in the previous section. In the last section, we encountered code that looked like the following:\n",
    "\n",
    "```python\n",
    "import fugue.api as fa\n",
    "\n",
    "df = fa.load(\"/tmp/f.parquet\", engine=\"dask\")\n",
    "res = fa.transform(df, dummy, schema=\"*\", engine=\"dask\")\n",
    "fa.save(res, \"/tmp/f_out.parquet\", engine=\"dask\")\n",
    "```\n",
    "\n",
    "We had to repeat the engine multiple times. To simplify this, we can use the `engine_context` to set a default execution engine for all of the Fugue API functions used inside. For example, all of the Fugue functions below will run on the Dask engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaskDataFrame\n",
      "a:long\n",
      "------\n",
      "1     \n",
      "2     \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fugue.api as fa \n",
    "\n",
    "df = pd.DataFrame({\"a\": [1,2]})\n",
    "df.to_parquet(\"/tmp/f.parquet\")\n",
    "\n",
    "def dummy(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    return df\n",
    "\n",
    "with fa.engine_context(\"dask\"):\n",
    "    df = fa.load(\"/tmp/f.parquet\")\n",
    "    res = fa.transform(df, dummy, schema=\"*\")\n",
    "    fa.show(res)\n",
    "    fa.save(res, \"/tmp/f_out.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output of the `show()` function, we can see that Dask was used to execute the operations. Using the `engine_context()` is not necessarily required, but it can heavily simplify the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding the Engine\n",
    "\n",
    "The `engine_context()` just sets a default engine, so it can be overridden if needed. In the example below, we use `engine=None` to use Pandas, but we'll specify the engine for the `transform()` call because it may be compute intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaskDataFrame\n",
      "a:long\n",
      "------\n",
      "1     \n",
      "2     \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with fa.engine_context(engine=None):\n",
    "    df = fa.load(\"/tmp/f.parquet\")\n",
    "    res = fa.transform(df, dummy, schema=\"*\", engine=\"dask\")\n",
    "    fa.show(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we passed no engine to the `engine_context`, the Dask engine was used in the `transform()` step and returned a Dask DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and engine_context()\n",
    "\n",
    "The same behavior will apply if a Python function calls Fugue API functions. This allows for grouping of logic into engine-agnostic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaskDataFrame\n",
      "a:long\n",
      "------\n",
      "1     \n",
      "2     \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def logic():\n",
    "    df = fa.load(\"/tmp/f.parquet\")\n",
    "    res = fa.transform(df, dummy, schema=\"*\")\n",
    "    fa.show(res)\n",
    "\n",
    "with fa.engine_context(\"dask\"):\n",
    "    logic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also wrap the whole `engine_context()` block under a function and pass in the engine. The output DataFrame will follow the engine passed. In the example below, a Dask DataFrame is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "def logic(engine):\n",
    "    with fa.engine_context(engine):\n",
    "        df = fa.load(\"/tmp/f.parquet\")\n",
    "        res = fa.transform(df, dummy, schema=\"*\")\n",
    "    return res\n",
    "\n",
    "\n",
    "out = logic(\"dask\")\n",
    "print(type(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Python Code\n",
    "\n",
    "The code inside the `engine_context()` is not limited to Fugue API functions. For example, loops can be used if an operation is being used multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:long|x:long\n",
      "------+------\n",
      "1     |16    \n",
      "2     |16    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue.column import col, lit\n",
    "\n",
    "with fa.engine_context():\n",
    "    df = fa.load(\"/tmp/f.parquet\")\n",
    "    df = fa.assign(df, x=lit(1))\n",
    "    for i in range(4):\n",
    "        df = fa.assign(df, x=col(\"x\")*lit(2))\n",
    "    fa.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoupling of Logic and Execution\n",
    "\n",
    "This section illustrates how to piece together end-to-end workflows that can then we run on Pandas, Spark, Dask, or Ray. The logic is fully decoupled from the execution, which is one of the primary motivations of Fugue. This solves the following problems:\n",
    "\n",
    "1. Users have to learn an entirely new framework to work with distributed computing problems\n",
    "2. Logic written for a *small data* project is not reusable for a *big data* project\n",
    "3. Testing becomes a heavyweight process for distributed computing, especially Spark\n",
    "4. Along with number 3, iterations for distributed computing problems become slower and more expensive\n",
    "\n",
    "Fugue's core principle is to minimize code dependency on frameworks as much as possible, leading to flexibility and portability. **By decoupling logic and execution, we can focus on our logic in a scale-agnostic way.** In this section, we saw how to build end-to-end workflows with the Fugue API and the `engine_context()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section we covered the `engine_context()` function, which sets the default execution engine for Fugue function calls. By wrapping it or using it alongside functions, it will be easier to group pieces of logic together to form framework-agnostic workloads. This can also be extended to create workflows that utilize different engines. A common use case is heavy processing with Spark, Dask, or Ray, and then doing post-processing with Pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
