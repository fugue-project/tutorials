{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fugue Interface\n",
    "\n",
    "In the previous section, we talked about the motivation of Fugue and showed a simple example of Fugue code. Here we provide more details around the Fugue syntax that will be needed in actual applications. We will also talk about some of the underlying concepts that were shown in the previous section, but not really explained.\n",
    "\n",
    "## The Directed Acyclic Graph (DAG)\n",
    "\n",
    "In the last section, we had a code block that used `with FugueWorkflow() as dag:`. We'll make another simple example below. Here we have a simple `transformer` that takes in two columns and adds them to create a third column called `col3`. This is then used in the `FugueWorkflow`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "data = pd.DataFrame({'col1': [1,2,3], 'col2':[2,3,4]})\n",
    "data2 = data.copy()\n",
    "\n",
    "# schema: *, col3:int\n",
    "def make_new_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2']\n",
    "    return df \n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data2)\n",
    "    df = df.transform(make_new_col)\n",
    "    df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "col1:long|col2:long|col3:int\n",
      "---------+---------+--------\n",
      "1        |2        |3       \n",
      "2        |3        |5       \n",
      "3        |4        |7       \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nothing here is new. This should all be familiar last section, however we did not really dive into what the `FugueWorkflow` actually does. The `FugueWorkflow` is responsible for constructing a Directed Acyclic Graph, also called a DAG. A lot of people associate the DAG concept with workflow orchestration tools like Airflow, Prefect, of Dagster. While these tools also use DAGs, they use it in a different way than the distributed compute frameworks (Spark and Dask). For orchestration frameworks, the DAG is used to manage dependencies of scheduled tasks. For compute frameworks, the DAG represents a computation graph that is built, validated, and then executed. DAGs are used because distributed compute operations are very expensive and have a lot of room to be optimized. Also, mistakes in a distributed setting are very expensive.\n",
    "\n",
    "Fugue follows these distributed compute frameworks in using the DAG for validation before execution. DAGs can catch errors significantly earlier, similar to compiling the compute job. For Fugue specifically, the built DAG validates schema, as well as provides the basis for further optimizations. For example, Fugue can detect which DataFrames are re-used in the computation graph, and then persist them automatically to avoid recomputation. The DAG is a graph where the nodes are DataFrames connected by Fugue extensions. We already introduced the most common extension, which is the `transformer`. Schema is tracked throughout the DAG. More extensions will be introduced later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schema\n",
    "\n",
    "This leads us to schema. In this context, schema maps column fields with their corresponding data types. Schema is explicit in Fugue for a couple of reasons:\n",
    "\n",
    "1. It allows quick validation if the computation job contains all of the necessary columns.\n",
    "\n",
    "2. It guarantees that operations are performed as expected across all machines. As a DataFrame can be split across multiple machines each machine only sees the data it holds. A strong schema prevents an operation intended for say an integer column from unexpectedly having to deal with string values.\n",
    "\n",
    "3. Inferring schema is an expensive and error-prone operation. To infer the schema, distributed computing frameworks have to go through at least one partition of data to figure out the possible schema. If the inferred schema is inconsistent across partitions, the result will be wrong.\n",
    "\n",
    "4. Schema is a requirement for using Spark and Dask"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Method 1: Schema Hint**\n",
    "\n",
    "In this case the comment is read and enforced by `FugueWorkflow`. This is the least invasive to code, and is not even dependent on Fugue. If a user chooses to move away from Fugue, these are still helpful comments that can remain in the code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# schema: *, col3:int\n",
    "def make_new_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2']\n",
    "    return df "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Method 2: Decorator**\n",
    "\n",
    "One of the limitations of the schema hint is that linters often complain if there is a very long schema (past 70 or 80 characters). In that situation, users can import a long string into their script and pass it to the `transformer` decorator. This is also more explicit that this function is being wrapped into a Fugue transformer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from fugue import transformer\n",
    "\n",
    "@transformer(schema=\"*, col3:int\")\n",
    "def make_new_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2']\n",
    "    return df "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Method 3: FugueWorkflow**\n",
    "\n",
    "If users don't want to use any of the above options, they can provide the schema in the `transform` call under the FugueWorkflow context manager like the example below. This converts the function to a `transformer` during execution. This leaves the original function untouched and only brings it to Spark or Dask during runtime when needed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data2 = data.copy()\n",
    "\n",
    "def make_new_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2']\n",
    "    return df \n",
    "    \n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data2)\n",
    "    df = df.transform(make_new_col, schema=\"*, col3:int\")\n",
    "    df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "col1:long|col2:long|col3:int\n",
      "---------+---------+--------\n",
      "1        |2        |3       \n",
      "2        |3        |5       \n",
      "3        |4        |7       \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passing Parameters\n",
    "\n",
    "We saw in the first section how to pass parameters into the `transform` function but we haven't seen how to do this yet with `FugueWorkflow`. Passing parameters is identical in both approaches. We pass them as a dictionary when we use the `transform` method. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data2 = data.copy()\n",
    "\n",
    "# schema: *, col3:int\n",
    "def make_new_col(df: pd.DataFrame, n=1) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2'] + n\n",
    "    return df \n",
    "    \n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data2)\n",
    "    df = df.transform(make_new_col, params={'n': 10})  # Pass parameters\n",
    "    df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "col1:long|col2:long|col3:int\n",
      "---------+---------+--------\n",
      "1        |2        |13      \n",
      "2        |3        |15      \n",
      "3        |4        |17      \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Saving Data\n",
    "\n",
    "Load and save operations are done inside the `FugueWorkflow` and use the appropriate saver/loader for the file extension (.csv, .json, .parquet, .avro) and ExecutionEngine (Pandas, Spark, or Dask). For distributed compute, parquet and avro tend to be the most used because of compression. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data2)\n",
    "    df.save('/tmp/data.parquet', mode=\"overwrite\", single=True)\n",
    "    df.save(\"/tmp/data.csv\", mode=\"overwrite\", header=True)\n",
    "    df2 = dag.load('/tmp/data.parquet')\n",
    "    df3 = dag.load(\"/tmp/data.csv\", header=True, columns=\"col1:int,col2:int\")\n",
    "    df3.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PandasDataFrame\n",
      "col1:int|col2:int\n",
      "--------+--------\n",
      "1       |2       \n",
      "2       |3       \n",
      "3       |4       \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this section we covered some conepts such as the DAG and why explicit schema is needed. We also covered how to define schema and pass in parameters. Combined with loading and saving of files, users can already start using Fugue for working with data."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('fugue-tutorials': conda)",
   "metadata": {
    "interpreter": {
     "hash": "131b24c7e1bb8763ab2b04d5b6d98a68c7b3a823a2a57c5722935f7690890f70"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}