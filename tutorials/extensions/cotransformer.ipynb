{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoTransformer\n",
    "\n",
    "`Transformer` represents the logic unit executing on arbitrary machine on a collection of partitions of the same partition keys of the input dataframes. The partitioning logic is not a concern of `CoTransformer`, it must be specified by `zip` in the previous step. You must understand [partition](partition.ipynb) and [zip](./execution_engine.ipynb#Zip-&-Comap)\n",
    "\n",
    "**Input can be a single** `DataFrames`\n",
    "\n",
    "**Alternatively it accepts input DataFrame types**: `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Output DataFrame types can be**: `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "Notice that `ArrayDataFrame` and other local dataframes can't be used as annotation, you must use `LocalDataFrame`.\n",
    "\n",
    "`CoTransformer` requires users to be explicit on the output schema. Different from `Transformer`, `*` is not allowed.\n",
    "\n",
    "## Why Explicit on Output Schema?\n",
    "\n",
    "Normally computing frameworks can infer output schema, however, it is neither reliable nor efficient. To infer the schema, it has to go through at least one partition of data and figure out the possible schema. However, what if a transformer is producing inconsistent schemas on different data partitions? What if that partition takes a long time or fail? So to avoid potential correctness and performance issues, `Transformer` and `CoTransformer` output schemas are required in Fugue.\n",
    "\n",
    "## Native Approach\n",
    "\n",
    "The simplest way, with no dependency on Fugue. You just need to have acceptable annotations on input dataframes and output. In native approach, you must specify schema in the Fugue code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "def to_str(df1:List[List[Any]], df2:List[Dict[str,Any]], n=1) -> List[List[Any]]:\n",
    "    return [[df1.__repr__(),df2.__repr__()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1],[1,3]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,4],[1,2]],\"a:int,c:int\")\n",
    "    df3 = dag.df([[0,2],[1,1],[1,5]],\"a:int,b:int\")\n",
    "    # with out schema hint you have to specify schema in Fugue code\n",
    "    # must have zip, by default, zip inner joins them by their common keys\n",
    "    df1.zip(df2).transform(to_str, schema=\"df1:str,df2:str\").show()\n",
    "    # if you don't want Fugue to infer the join keys, you can specify\n",
    "    df1.zip(df3, partition={\"by\":\"a\"}).transform(to_str, schema=\"df1:str,df2:str\").show()\n",
    "    # you can also presort partitions\n",
    "    df1.zip(df3, partition={\"by\":\"a\", \"presort\":\"b DESC\"}).transform(to_str, schema=\"df1:str,df2:str\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Schema Hint\n",
    "\n",
    "When you need to reuse a cotransformer multiple times, it's tedious to specify the schema in Fugue code every time. You can instead, write a schema hint on top of the function, this doesn't require you to have Fugue dependency. The following code is doing the same thing as above but see how much shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "#schema: df1:str,df2:str\n",
    "def to_str(df1:List[List[Any]], df2:List[Dict[str,Any]], n=1) -> List[List[Any]]:\n",
    "    return [[df1.__repr__(),df2.__repr__()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1],[1,3]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,4],[1,2]],\"a:int,c:int\")\n",
    "    df3 = dag.df([[0,2],[1,1],[1,5]],\"a:int,b:int\")\n",
    "    df1.zip(df2).transform(to_str).show()\n",
    "    df1.zip(df3, partition={\"by\":\"a\"}).transform(to_str).show()\n",
    "    df1.zip(df3, partition={\"by\":\"a\", \"presort\":\"b DESC\"}).transform(to_str).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataFrames\n",
    "\n",
    "Instead of using dataframes as input, you can use a single `DataFrames` for arbitrary number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "from fugue import DataFrames, FugueWorkflow\n",
    "\n",
    "#schema: res:[str]\n",
    "def to_str(dfs:DataFrames) -> List[List[Any]]:\n",
    "    return [[[x.as_array().__repr__() for x in dfs.values()]]]\n",
    "\n",
    "#schema: res:[str]\n",
    "def to_str_with_key(dfs:DataFrames) -> List[List[Any]]:\n",
    "    return [[[k+\" \"+x.as_array().__repr__() for k,x in dfs.items()]]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1],[1,3]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,4],[1,2]],\"a:int,c:int\")\n",
    "    df3 = dag.df([[0,2],[1,1],[1,5]],\"a:int,d:int\")\n",
    "    dag.zip(df1,df2,df3).transform(to_str).show()\n",
    "    dag.zip(df1,df2,df3).transform(to_str_with_key).show()\n",
    "    dag.zip(dict(a=df1,b=df2,c=df3)).transform(to_str_with_key).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator Approach\n",
    "\n",
    "Decorator approach can do everything the schema hint can do, plus, it can take in a function to generate the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, Schema, cotransformer\n",
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "# dfs is the zipped DataFrames, **params is the parameters passed in from fugue\n",
    "def schema_from_dfs(dfs, **params):\n",
    "    return Schema([(\"_\".join(df.schema.names),str) for df in dfs.values()])\n",
    "    \n",
    "@cotransformer(schema_from_dfs)\n",
    "def to_str(df1:List[List[Any]], df2:List[Dict[str,Any]], n=1) -> List[List[Any]]:\n",
    "    return [[df1.__repr__(),df2.__repr__()]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1],[1,3]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,4],[1,2]],\"a:int,c:int\")\n",
    "    df1.zip(df2).transform(to_str).show() # see the output schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Approach\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of the use cases and simplify the usage. But for certain cases, you should implement interface, for example\n",
    "\n",
    "* You need partition information, such as partition keys, schema, and current values of the keys\n",
    "* You have an expensive but common initialization step for processing each logical partition, this should happen when initializaing physical partition\n",
    "\n",
    "The biggest advantage of interface approach is that you can customize pyhisical partition level initialization, and you have all the up-to-date context variables to use.\n",
    "\n",
    "In the interface approach, type annotations are not necessary, but again, it's good practice to have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fugue import Transformer, FugueWorkflow, DataFrame, LocalDataFrame, PandasDataFrame\n",
    "from fugue import CoTransformer, FugueWorkflow, PandasDataFrame, DataFrame, ArrayDataFrame\n",
    "from triad.collections import Schema\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def expensive_init(sec=5):\n",
    "    sleep(sec)\n",
    "\n",
    "def helper(ct=20) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 2)), columns=list('ab'))\n",
    "\n",
    "class Median(CoTransformer):\n",
    "    # this is invoked on driver side\n",
    "    def get_output_schema(self, dfs):\n",
    "        return self.key_schema + [k+\":double\" for k in dfs.keys()]\n",
    "    \n",
    "    # on initialization of the physical partition\n",
    "    def on_init(self, df: DataFrame) -> None:\n",
    "        expensive_init(self.params.get(\"sec\",0))\n",
    "        \n",
    "    def transform(self, dfs):\n",
    "        result = self.cursor.key_value_array\n",
    "        for k, v in dfs.items():\n",
    "            m = v.as_pandas()[\"b\"].median()\n",
    "            result.append(m)\n",
    "        return ArrayDataFrame([result], self.output_schema)\n",
    "        \n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    # a, b are identical because of the seed\n",
    "    a=dag.create(helper)\n",
    "    b=dag.create(helper)\n",
    "    dag.zip(dict(x=a,y=b), partition={\"by\":[\"a\"]}).transform(Median, params={\"sec\": 1}).show(rows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a few things here:\n",
    "\n",
    "* How we access the key schema (`self.key_schema`), and current logical partition's keys as array (`self.cursor.key_value_array`)\n",
    "* Although DataFrames is a dict, it's an ordered dict following the input order, so you can iterate in this way\n",
    "* `expensive_init` is something that is a common initialization for different logical partitions, we move it to `on_init` so it will run once for each physcial partition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}