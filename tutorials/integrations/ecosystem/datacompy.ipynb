{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataComPy\n",
    "\n",
    "Have questions? Chat with us on Github or Slack:\n",
    "\n",
    "[![Homepage](https://img.shields.io/badge/fugue-source--code-red?logo=github)](https://github.com/fugue-project/fugue)\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "[DataComPy](https://github.com/capitalone/datacompy) is an open-source package by Capital One that started as a way to compare two Pandas DataFrames with some more functionality than just Pandas.DataFrame.equals(Pandas.DataFrame). It allows users to specify tolerances and prints out statistics.\n",
    "\n",
    "Fugue is now an internal dependency of DataCompy, which extends the functionality to be used on backends Fugue supports (Spark, Dask, Ray, Polars, DuckDB, Arrow, etc.). A common use case is also comparing a Pandas DataFrame with a distributed DataFrame (Spark, Dask or Ray)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "The most scalable way to use DataCompy is the `is_match` method. An example can be found below. The DataFrames are joined on the `acct_id` column and then compared. There are other supported operations not covered here. For more details, check the [DataCompy documentation](https://capitalone.github.io/datacompy/#things-that-are-happening-behind-the-scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import datacompy\n",
    "\n",
    "data1 = \"\"\"acct_id,dollar_amt,name,float_fld,date_fld\n",
    "10000001234,123.45,George Maharis,14530.1555,2017-01-01\n",
    "10000001235,0.45,Michael Bluth,1,2017-01-01\n",
    "10000001236,1345,George Bluth,,2017-01-01\n",
    "10000001237,123456,Bob Loblaw,345.12,2017-01-01\n",
    "10000001239,1.05,Lucille Bluth,,2017-01-01\n",
    "\"\"\"\n",
    "\n",
    "data2 = \"\"\"acct_id,dollar_amt,name,float_fld\n",
    "10000001234,123.4,George Michael Bluth,14530.155\n",
    "10000001235,0.45,Michael Bluth,\n",
    "10000001236,1345,George Bluth,1\n",
    "10000001237,123456,Robert Loblaw,345.12\n",
    "10000001238,1.05,Loose Seal Bluth,111\n",
    "\"\"\"\n",
    "\n",
    "df1 = pd.read_csv(StringIO(data1))\n",
    "df2 = pd.read_csv(StringIO(data2))\n",
    "\n",
    "datacompy.is_match(\n",
    "    df1,\n",
    "    df2,\n",
    "    join_columns='acct_id',  #You can also specify a list of columns\n",
    "    abs_tol=0, #Optional, defaults to 0\n",
    "    rel_tol=0, #Optional, defaults to 0\n",
    "    df1_name='Original', #Optional, defaults to 'df1'\n",
    "    df2_name='New' #Optional, defaults to 'df2'\n",
    ")\n",
    "# False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Generation\n",
    "\n",
    "For a full report, use the `report` function. The report is truncated in this notebook because the output is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataComPy Comparison\n",
      "--------------------\n",
      "\n",
      "DataFrame Summary\n",
      "-----------------\n",
      "\n",
      "  DataFrame  Columns  Rows\n",
      "0  Original        5     5\n",
      "1       New        4     5\n",
      "\n",
      "Column Summary\n",
      "--------------\n",
      "\n",
      "Number of columns in common: 4\n",
      "Number of columns in Original but not in New: 1\n",
      "Number of columns in New but not in Original: 0\n",
      "\n",
      "Row Summary\n",
      "-----------\n",
      "\n",
      "Matched on: acct_id\n",
      "Any duplicates on match values: No\n",
      "Absolute Tolerance: 0\n",
      "Relative Tolerance: 0\n",
      "Number of rows in common: 4\n",
      "Number of rows in Original but not in New: 1\n",
      "Number of rows in New but not in Original: 1\n",
      "\n",
      "Number of rows with some compared columns unequal: 4\n",
      "Number of rows with all compared columns equal: 0\n",
      "\n",
      "Column Comparison\n",
      "-----------------\n",
      "\n",
      "Number of columns compared with some values unequal: 3\n",
      "Number of columns compared with all values equal: 1\n",
      "Total number of values which compare unequal: 6\n",
      "\n",
      "Columns with Unequal Values or Types\n",
      "------------------------------------\n",
      "\n",
      "       Column Original dtype New dtype  # Unequal  Max Diff  # Null Diff\n",
      "0  dollar_amt        float64   float64          1    0.0500            0\n",
      "2   float_fld        float64   float64          3    0.0005            2\n",
      "1        name         object    object          2    0.0000            0\n",
      "\n",
      "Sample Rows with Unequal Values\n",
      "-------------------------------\n",
      "\n",
      "       acct_id  dollar_amt (Original)  dollar_amt (New)\n",
      "0  10000001234                 123.45             123.4\n",
      "\n",
      "       acct_id name (Original)            name (New)\n",
      "0  10000001237      Bob Loblaw         Robert Loblaw\n",
      "1  10000001234  George Maharis  George Michael Bluth\n",
      "\n",
      "       acct_id  float_fld (Original)  float_fld (New)\n",
      "0  10000001234            14530.1555        14530.155\n",
      "1  10000001236                   NaN            1.000\n",
      "2  10000001235                1.0000              NaN\n",
      "\n",
      "Sample Rows Only in Original (First 10 Columns)\n",
      "-----------------------------------------------\n",
      "\n",
      "       acct_id  dollar_amt           name  float_fld    date_fld\n",
      "0  10000001239        1.05  Lucille Bluth        NaN  2017-01-01\n",
      "\n",
      "Sample Rows Only in New (First 10 Columns)\n",
      "------------------------------------------\n",
      "\n",
      "       acct_id  dollar_amt              name  float_fld\n",
      "0  10000001238        1.05  Loose Seal Bluth      111.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This method prints out a human-readable report summarizing and sampling differences\n",
    "print(datacompy.report(\n",
    "    df1,\n",
    "    df2,\n",
    "    join_columns='acct_id',  #You can also specify a list of columns\n",
    "    abs_tol=0, #Optional, defaults to 0\n",
    "    rel_tol=0, #Optional, defaults to 0\n",
    "    df1_name='Original', #Optional, defaults to 'df1'\n",
    "    df2_name='New' #Optional, defaults to 'df2'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Usage\n",
    "\n",
    "In order to compare DataFrames of different backends, you just need to replace df1 and df2 with DataFrames of different backends. Just pass in DataFrames such as Pandas DataFrames, DuckDB relations, Polars DataFrames, Arrow tables, Spark DataFrames, Dask DataFrames or Ray Datasets. For example, to compare a Pandas DataFrame with a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_df2 = spark.createDataFrame(df2)\n",
    "datacompy.is_match(\n",
    "    df1,\n",
    "    spark_df2,\n",
    "    join_columns='acct_id',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in order to use a specific backend, you need to have the corresponding library installed. For example, if you want compare Ray datasets, you must do:\n",
    "\n",
    "```\n",
    "pip install datacompy[ray]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we introduced the DataComPy library and showed how to compare DataFrames across Spark, Dask, Ray, DuckDB, PyArrow, and Polars through Fugue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
