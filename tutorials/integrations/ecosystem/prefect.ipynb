{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6941338-00c2-4a34-a3f6-11e84ee80500",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prefect\n",
    "\n",
    "[Prefect](https://docs.prefect.io/) is an open-source workflow orchestration framework designed for the modern data stack. The `prefect-fugue` collection allows users to prototype their code locally, and then execute it on a distributed computing cluster (Spark, Dask, Ray) when production ready. It also lets users submit their tasks to a Databricks cluster easily, even without Fugue.\n",
    "\n",
    "The `prefect-fugue` collection runs on top of the `fugue-cloudprovider` library which can run on top of [Databricks](https://www.databricks.com/), [Coiled](https://coiled.io/) and [Anyscale](https://www.anyscale.com/). For supported cloud providers, see the [Cloud Providers](../cloudproviders/index.md) section of the tutorials.\n",
    "\n",
    "If you use some other platform such as self-hosted Spark, Dask, or Ray. You can still take advantage of the `prefect-fugue` collection by passing a `SparkSession`, `Dask Client` or Ray cluster address. This tutorial focuses on Spark specifically because Prefect already has Dask and Ray support in `prefect-dask` and `prefect-ray` for running tasks. Still, this may be a good approach when working with DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef2e99",
   "metadata": {},
   "source": [
    "## Working with Fugue Blocks\n",
    "\n",
    "The `prefect-fugue` collection uses Prefect [Blocks](https://docs.prefect.io/ui/blocks/) to hold connections to compute. The first step would be to register a `Fugue Engine` block. First we install the collection:\n",
    "\n",
    "```bash\n",
    "pip install prefect-fugue\n",
    "```\n",
    "\n",
    "and then we can register the block into our workspace so it will appear in the Prefect UI.\n",
    "\n",
    "```bash\n",
    "prefect block register -m prefect_fugue\n",
    "```\n",
    "\n",
    "That will display the block in our Prefect workspace. Below is what the creation screen will look like.\n",
    "\n",
    "![img](../../../images/prefect_fugue_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39e16f",
   "metadata": {},
   "source": [
    "There are 4 items that need to be filled to create a block.\n",
    "\n",
    "1. Block Name - name that will be used to use the block.\n",
    "2. Engine Name - one of the Fugue supported backends (spark, dask, ray, duckdb)\n",
    "3. Engine Config - configurations related to the cluster\n",
    "4. Secret Config - credentials to connect to a cluster\n",
    "\n",
    "In this example, we create a block to use a Databricks cluster.\n",
    "\n",
    "1. Block Name - databricks\n",
    "2. Engine Name - spark\n",
    "3. Engine Config - None\n",
    "4. Secret Config - seen below\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"host\": \"https://dbc-38aaa459-faaf.cloud.databricks.com\",\n",
    "    \"token\": \"dapiecaaae64a727498daaaaafe1bace968a\",\n",
    "    \"cluster_id\": \"0612-191111-6fopaaaa\"\n",
    "}\n",
    "```\n",
    "\n",
    "More information can be found on the `databricks` section under the `cloudprovider` [tutorials](https://fugue-tutorials.readthedocs.io/tutorials/integrations/cloudproviders/databricks.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd35b0-58d8-41ce-9f24-1812b1cc7f05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using a Spark Cluster Inside a Flow\n",
    "\n",
    "Let's start by running code on top of Databricks. `databricks-connect` is already installed in this environment. This section may have a lot of logs because of the monitoring provided by Prefect. This section also assumes that the user has Prefect configured to the right workspace.\n",
    "\n",
    "Below we have one task that takes in a `SparkSession` and uses it to run some Spark code. We can then use this in the Prefect Flow with the `fugue_engine` context. This `fugue_engine` will create an ephemeral cluster to run the code underneath, and then turn off when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41883a88-339e-430d-92d4-8e93c5de74db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prefect import task, flow\n",
    "from prefect_fugue import fugue_engine\n",
    "\n",
    "@task\n",
    "def my_spark_task(spark, n=1):\n",
    "    df = spark.createDataFrame([[f\"hello spark {n}\"]], \"a string\")\n",
    "    df.show()\n",
    "\n",
    "@flow\n",
    "def native_spark_flow(engine):\n",
    "    with fugue_engine(engine) as engine:\n",
    "        my_spark_task(engine.spark_session, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ef557",
   "metadata": {},
   "source": [
    "Then now we can invoke this Flow using the Fugue block as the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f423c5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:14:01.381 | INFO    | prefect.engine - Created flow run 'belligerent-potoo' for flow 'native-spark-flow'\n",
      "22:14:02.167 | INFO    | Flow run 'belligerent-potoo' - Created task run 'my_spark_task-4b707fb5-0' for task 'my_spark_task'\n",
      "22:14:02.169 | INFO    | Flow run 'belligerent-potoo' - Executing 'my_spark_task-4b707fb5-0' immediately...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:14:03.273 | INFO    | Task run 'my_spark_task-4b707fb5-0' - Finished in state Completed()\n",
      "22:14:03.361 | INFO    | Flow run 'belligerent-potoo' - Finished in state Completed('All states completed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|            a|\n",
      "+-------------+\n",
      "|hello spark 1|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Completed(message=None, type=COMPLETED, result=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_spark_flow(\"fugue/databricks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d6f8c",
   "metadata": {},
   "source": [
    "Similarly, if you don't use Databricks but have your own way to get a `SparkSession`, you can directly pass the `SparkSession` into the Flow. The `fugue_engine` context will be able to interpret this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490eada9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:14:07.803 | INFO    | prefect.engine - Created flow run 'rational-alligator' for flow 'native-spark-flow'\n",
      "22:14:08.453 | INFO    | Flow run 'rational-alligator' - Created task run 'my_spark_task-4b707fb5-0' for task 'my_spark_task'\n",
      "22:14:08.453 | INFO    | Flow run 'rational-alligator' - Executing 'my_spark_task-4b707fb5-0' immediately...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:14:09.408 | INFO    | Task run 'my_spark_task-4b707fb5-0' - Finished in state Completed()\n",
      "22:14:09.498 | INFO    | Flow run 'rational-alligator' - Finished in state Completed('All states completed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|            a|\n",
      "+-------------+\n",
      "|hello spark 1|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Completed(message=None, type=COMPLETED, result=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "native_spark_flow(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1e16d-dac8-4611-aafe-10541c88af40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Map Jobs on Spark/Dask with 0 Spark/Dask code\n",
    "\n",
    "We showed how to run Spark code on top of a Spark cluster, but the strength of Fugue is decoupling from distributed framework code such as Spark and Dask. Decoupling from these frameworks allows us to test code locally before scaling out to a cluster. In the example below, we simulate having a pandas DataFrame where each row is a job. \n",
    "\n",
    "When testing the Flow, we can pass `None` as the engine so everything runs on Pandas. When ready to scale out, we can pass in our `Block` or `SparkSession`. Fugue's `transform()` task will use the engine provided by the `fugue_engine` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a7556c-cc33-4529-b0de-5cc690b43507",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import pandas as pd\n",
    "from prefect_fugue import transform\n",
    "\n",
    "@task\n",
    "def create_jobs(n) -> pd.DataFrame:\n",
    "    return pd.DataFrame(dict(jobs=range(n)))\n",
    "\n",
    "# schema: *,batch_size:str\n",
    "def run_one_job(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    sleep(len(df)*5)\n",
    "    return df.assign(batch_size=len(df))\n",
    "\n",
    "@flow\n",
    "def run_all_jobs(n, engine=None):\n",
    "    jobs = create_jobs(n)\n",
    "    with fugue_engine(engine):\n",
    "        return transform(jobs, run_one_job, partition=\"per_row\", as_local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fb78a",
   "metadata": {},
   "source": [
    "We can test the Flow above on a local machine without Spark. We run on one job first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e84d2f-6ead-4a6a-afc3-0f5680725674",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:25:46.575 | INFO    | prefect.engine - Created flow run 'hopping-curassow' for flow 'run-all-jobs'\n",
      "22:25:47.401 | INFO    | Flow run 'hopping-curassow' - Created task run 'create_jobs-71a807e0-0' for task 'create_jobs'\n",
      "22:25:47.402 | INFO    | Flow run 'hopping-curassow' - Executing 'create_jobs-71a807e0-0' immediately...\n",
      "22:25:47.653 | INFO    | Task run 'create_jobs-71a807e0-0' - Finished in state Completed()\n",
      "22:25:47.772 | INFO    | Flow run 'hopping-curassow' - Created task run 'run_one_job (transfomer) - f6d46-1e345475-0' for task 'run_one_job (transfomer) - f6d46'\n",
      "22:25:47.774 | INFO    | Flow run 'hopping-curassow' - Executing 'run_one_job (transfomer) - f6d46-1e345475-0' immediately...\n",
      "22:25:47.974 | WARNING | root - NativeExecutionEngine doesn't respect num_partitions ROWCOUNT\n",
      "22:25:53.150 | INFO    | Task run 'run_one_job (transfomer) - f6d46-1e345475-0' - Finished in state Completed()\n",
      "22:25:53.256 | INFO    | Flow run 'hopping-curassow' - Finished in state Completed()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobs</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jobs batch_size\n",
       "0     0          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_all_jobs(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4998b",
   "metadata": {},
   "source": [
    "Becasue it succeeded, we can now attach our Fugue Databricks `Block` to run on Databricks. Now we run on 8 jobs, and we'll see that parallelization from the Spark cluster will make this Flow execute faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1af4e86-2991-43b1-838c-43ff6ff4b8df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:38:07.115 | INFO    | prefect.engine - Created flow run 'observant-doberman' for flow 'run-all-jobs'\n",
      "22:38:07.747 | INFO    | Flow run 'observant-doberman' - Created task run 'create_jobs-71a807e0-0' for task 'create_jobs'\n",
      "22:38:07.749 | INFO    | Flow run 'observant-doberman' - Executing 'create_jobs-71a807e0-0' immediately...\n",
      "22:38:07.963 | INFO    | Task run 'create_jobs-71a807e0-0' - Finished in state Completed()\n",
      "22:38:08.239 | INFO    | Flow run 'observant-doberman' - Created task run 'run_one_job (transfomer) - 0f7fe-1e345475-0' for task 'run_one_job (transfomer) - 0f7fe'\n",
      "22:38:08.241 | INFO    | Flow run 'observant-doberman' - Executing 'run_one_job (transfomer) - 0f7fe-1e345475-0' immediately...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n",
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n",
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n",
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n",
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:38:29.844 | INFO    | Task run 'run_one_job (transfomer) - 0f7fe-1e345475-0' - Finished in state Completed()\n",
      "22:38:29.929 | INFO    | Flow run 'observant-doberman' - Finished in state Completed()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 332 ms, sys: 56.2 ms, total: 389 ms\n",
      "Wall time: 23.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobs</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jobs batch_size\n",
       "0     0          1\n",
       "1     1          1\n",
       "2     2          1\n",
       "3     3          1\n",
       "4     4          1\n",
       "5     5          1\n",
       "6     6          1\n",
       "7     7          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run_all_jobs(8, \"fugue/databricks\") # run on databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6f108",
   "metadata": {},
   "source": [
    "There is still some overhead with sending the work, but the time is decreased compared to the expected execution time if ran sequentially (40 seconds).\n",
    "\n",
    "We can also use local Dask by passing the string `\"dask\"`. We can also pass a `Dask Client()` or use the Fugue Engine `Block` with Coiled. More information can be found in the [Coiled cloudprovider docs](../cloudproviders/coiled.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "042e580d-ede4-4963-b051-c98abf1a96e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:39:43.381 | INFO    | prefect.engine - Created flow run 'imported-mammoth' for flow 'run-all-jobs'\n",
      "22:39:43.944 | INFO    | Flow run 'imported-mammoth' - Created task run 'create_jobs-71a807e0-0' for task 'create_jobs'\n",
      "22:39:43.946 | INFO    | Flow run 'imported-mammoth' - Executing 'create_jobs-71a807e0-0' immediately...\n",
      "22:39:44.164 | INFO    | Task run 'create_jobs-71a807e0-0' - Finished in state Completed()\n",
      "22:39:44.258 | INFO    | Flow run 'imported-mammoth' - Created task run 'run_one_job (transfomer) - 71d77-1e345475-0' for task 'run_one_job (transfomer) - 71d77'\n",
      "22:39:44.260 | INFO    | Flow run 'imported-mammoth' - Executing 'run_one_job (transfomer) - 71d77-1e345475-0' immediately...\n",
      "22:39:49.663 | INFO    | Task run 'run_one_job (transfomer) - 71d77-1e345475-0' - Finished in state Completed()\n",
      "22:39:49.741 | INFO    | Flow run 'imported-mammoth' - Finished in state Completed()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 768 ms, sys: 189 ms, total: 956 ms\n",
      "Wall time: 6.63 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobs</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jobs batch_size\n",
       "0     0          1\n",
       "1     1          1\n",
       "2     2          1\n",
       "3     3          1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "run_all_jobs(4, \"dask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763d45b-e2e1-4007-baf9-f5eefb87ce17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running SQL on any Spark, Dask, and Duckdb\n",
    "\n",
    "Prototyping locally, and then running the full job on the cluster is also possible with [FugueSQL](../../quick_look/ten_minutes_sql.ipynb). DuckDB is a good engine to run SQL queries on flat files or Pandas DataFrames. When ready, we can bring it to SparkSQL on the cluster. Similar to the `transform()` task shown above, there is also an `fsql()` task.\n",
    "\n",
    "Here we can load in data and perform a query with FugueSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "072f9884-d326-4ec3-952d-e0033731e484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prefect_fugue import fsql\n",
    "\n",
    "@task\n",
    "def load_data():\n",
    "    return pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\")\n",
    "\n",
    "@flow\n",
    "def run_sql(top, engine):\n",
    "    data = load_data()\n",
    "    with fugue_engine(engine):\n",
    "        fsql(\"\"\"\n",
    "        SELECT PULocationID, COUNT(*) AS ct FROM df\n",
    "        GROUP BY 1 ORDER BY 2 DESC LIMIT {{top}}\n",
    "        PRINT\n",
    "        \"\"\", df=data, top=top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9ab4f",
   "metadata": {},
   "source": [
    "To debug locally without SparkSQL, we can use DuckDB as the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778a591b-40a7-4f56-aeef-ace791638fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:43:43.403 | INFO    | prefect.engine - Created flow run 'spectacular-mongrel' for flow 'run-sql'\n",
      "22:43:44.013 | INFO    | Flow run 'spectacular-mongrel' - Created task run 'load_data-2ff00c39-0' for task 'load_data'\n",
      "22:43:44.014 | INFO    | Flow run 'spectacular-mongrel' - Executing 'load_data-2ff00c39-0' immediately...\n",
      "22:43:44.769 | INFO    | Task run 'load_data-2ff00c39-0' - Finished in state Completed()\n",
      "22:43:44.881 | INFO    | Flow run 'spectacular-mongrel' - Created task run 'SELECT PULocationID, COUNT(*) AS ct FROM... - 12537-54db7018-0' for task 'SELECT PULocationID, COUNT(*) AS ct FROM... - 12537'\n",
      "22:43:44.883 | INFO    | Flow run 'spectacular-mongrel' - Executing 'SELECT PULocationID, COUNT(*) AS ct FROM... - 12537-54db7018-0' immediately...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>9728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>8152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PULocationID    ct\n",
       "0            74  9728\n",
       "1            75  8152"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: PULocationID:long,ct:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:43:45.537 | INFO    | Task run 'SELECT PULocationID, COUNT(*) AS ct FROM... - 12537-54db7018-0' - Finished in state Completed()\n",
      "22:43:45.832 | INFO    | Flow run 'spectacular-mongrel' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "run_sql(2, \"duckdb\"); # debug/develop without spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df57df6",
   "metadata": {},
   "source": [
    "Again to run on the cluster, we can use the Block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e603b16-e369-44ae-abdf-600004c9f50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:44:03.182 | INFO    | prefect.engine - Created flow run 'cooperative-shark' for flow 'run-sql'\n",
      "22:44:03.821 | INFO    | Flow run 'cooperative-shark' - Created task run 'load_data-2ff00c39-0' for task 'load_data'\n",
      "22:44:03.823 | INFO    | Flow run 'cooperative-shark' - Executing 'load_data-2ff00c39-0' immediately...\n",
      "22:44:04.424 | INFO    | Task run 'load_data-2ff00c39-0' - Finished in state Completed()\n",
      "22:44:04.689 | INFO    | Flow run 'cooperative-shark' - Created task run 'SELECT PULocationID, COUNT(*) AS ct FROM... - b3b18-54db7018-0' for task 'SELECT PULocationID, COUNT(*) AS ct FROM... - b3b18'\n",
      "22:44:04.691 | INFO    | Flow run 'cooperative-shark' - Executing 'SELECT PULocationID, COUNT(*) AS ct FROM... - b3b18-54db7018-0' immediately...\n",
      "22/08/30 22:44:11 WARN SparkServiceRPCClient: Syncing Temp Views took 6033 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n",
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>9728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>8152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>4904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>2635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95</td>\n",
       "      <td>2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97</td>\n",
       "      <td>2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>244</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PULocationID    ct\n",
       "0            74  9728\n",
       "1            75  8152\n",
       "2            41  4904\n",
       "3            42  2693\n",
       "4           166  2635\n",
       "5            95  2561\n",
       "6            97  2240\n",
       "7             7  2068\n",
       "8            43  1916\n",
       "9           244  1881"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: PULocationID:long,ct:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:44:16.945 | INFO    | Task run 'SELECT PULocationID, COUNT(*) AS ct FROM... - b3b18-54db7018-0' - Finished in state Completed()\n",
      "22:44:17.210 | INFO    | Flow run 'cooperative-shark' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "run_sql(10, \"fugue/databricks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf0861",
   "metadata": {},
   "source": [
    "The downside of the query above is that our data loading is still tied to the Pandas interface. FugueSQL has additional keywords such as `LOAD` and `SAVE` so we can run everything from loading, processing, and saving all on DuckDB or SparkSQL. More information on FugueSQL can be found in the FugueSQL section of the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "312eb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow\n",
    "def run_sql_full(top, engine):\n",
    "    with fugue_engine(engine):\n",
    "        fsql(\"\"\"\n",
    "        df = LOAD \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\"\n",
    "\n",
    "        SELECT PULocationID, COUNT(*) AS ct FROM df\n",
    "        GROUP BY 1 ORDER BY 2 DESC LIMIT {{top}}\n",
    "        PRINT\n",
    "        \"\"\", top=top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae6e784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:46:52.723 | INFO    | prefect.engine - Created flow run 'neat-junglefowl' for flow 'run-sql'\n",
      "22:46:53.309 | INFO    | Flow run 'neat-junglefowl' - Created task run 'load_data-2ff00c39-0' for task 'load_data'\n",
      "22:46:53.311 | INFO    | Flow run 'neat-junglefowl' - Executing 'load_data-2ff00c39-0' immediately...\n",
      "22:46:54.001 | INFO    | Task run 'load_data-2ff00c39-0' - Finished in state Completed()\n",
      "22:46:54.265 | INFO    | Flow run 'neat-junglefowl' - Created task run 'SELECT PULocationID, COUNT(*) AS ct FROM... - 133ce-54db7018-0' for task 'SELECT PULocationID, COUNT(*) AS ct FROM... - 133ce'\n",
      "22:46:54.266 | INFO    | Flow run 'neat-junglefowl' - Executing 'SELECT PULocationID, COUNT(*) AS ct FROM... - 133ce-54db7018-0' immediately...\n",
      "22/08/30 22:47:03 WARN SparkServiceRPCClient: Syncing Temp Views took 3078 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n",
      "View job details at https://dbc-fd3c27a3-594d.cloud.databricks.com/?o=2707332449367147#/setting/clusters/0804-155157-zq2rzdv4/sparkUi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>9728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>8152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PULocationID    ct\n",
       "0            74  9728\n",
       "1            75  8152"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: PULocationID:long,ct:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:47:06.503 | INFO    | Task run 'SELECT PULocationID, COUNT(*) AS ct FROM... - 133ce-54db7018-0' - Finished in state Completed()\n",
      "22:47:06.763 | INFO    | Flow run 'neat-junglefowl' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "run_sql(2, \"fugue/databricks\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
