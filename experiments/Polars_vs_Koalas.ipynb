{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12be1c7a-6999-4620-906a-9db73548abff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Fugue vs Pandas on Spark\n",
    "\n",
    "These experiments were run on Databricks Runtime 12.2 LTS with a cluster of 8 machines with 16 cpus each. We also used:\n",
    "\n",
    "* Fugue 0.8.2dev3\n",
    "* Polars 0.16.14\n",
    "* Spark 3.3\n",
    "\n",
    "## Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff6b566-c18b-49bd-8b7f-9a24e83807fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "from typing import List,Dict,Any,Iterable\n",
    "import fugue.api as fa\n",
    "from triad import Schema\n",
    "\n",
    "COLS = [\"_\"+str(x) for x in range(10)]\n",
    "PL_COLS = [pl.col(\"_\"+str(x)) for x in range(10)]\n",
    "SPARK_PATH = \"/dbfs/ht2-{n}.parquet\"\n",
    "WHOLE_PERIOD = 28*24\n",
    "PERIOD = 7*24\n",
    "\n",
    "def make_df(n, start, periods, freq):\n",
    "    ts = pd.date_range(start, periods=periods, freq=freq, name=\"ts\")\n",
    "    m = np.random.rand(len(ts),len(COLS))\n",
    "    df = pd.DataFrame(m, columns=COLS, index=ts).reset_index()\n",
    "    return df.assign(uid=n)\n",
    "\n",
    "def make_dfs(df:Iterable[List[Any]]) -> Iterable[pd.DataFrame]:\n",
    "    for row in df:\n",
    "        yield make_df(row[0], \"2000-01-01\", row[1], \"min\")\n",
    "\n",
    "# schema: *,greatest:double\n",
    "def greatest_pd(dfs:Iterable[pd.DataFrame]) -> Iterable[pd.DataFrame]:\n",
    "    for df in dfs:\n",
    "        yield df.assign(greatest=df[COLS].max(axis=1))\n",
    "\n",
    "# schema: *,greatest:double\n",
    "def greatest_pl(dfs:Iterable[pl.DataFrame]) -> Iterable[pl.DataFrame]:\n",
    "    for df in dfs:\n",
    "        yield df.with_columns(pl.max(PL_COLS).alias(\"greatest\"))\n",
    "        \n",
    "# schema:*\n",
    "def zscore_pd(df:pd.DataFrame, n) -> pd.DataFrame:\n",
    "    subdf = df[COLS]\n",
    "    df = df.sort_values(\"ts\")\n",
    "    x = subdf.shift(1).rolling(n)\n",
    "    z=(subdf-x.mean()).abs()/x.std()\n",
    "    return df.assign(**{k:z[k] for k in COLS}).dropna()\n",
    "\n",
    "# schema:*\n",
    "def zscore_pd_gp(df:pd.DataFrame, n) -> pd.DataFrame:\n",
    "    idf = df.set_index(\"uid\").sort_values([\"uid\",\"ts\"])\n",
    "    subdf = idf[COLS]\n",
    "    x = subdf.groupby(\"uid\").shift(1).rolling(n)\n",
    "    z=(subdf-x.mean()).abs()/x.std()\n",
    "    return idf.assign(**{k:z[k] for k in COLS}).dropna().reset_index()\n",
    "\n",
    "# schema:*\n",
    "def zscore_pl(df:pl.DataFrame, n:int) -> pl.DataFrame:\n",
    "    params = {}\n",
    "    for col in COLS:\n",
    "        mean = pl.col(col).shift().rolling_mean(n, min_periods=n)\n",
    "        std = pl.col(col).shift().rolling_std(n, min_periods=n)\n",
    "        params[col]=(pl.col(col) - mean).abs()/std\n",
    "    return df.sort(\"ts\").with_columns(**params).drop_nulls()\n",
    "\n",
    "# schema:*\n",
    "def zscore_pl_gp(df:pl.DataFrame, n:int) -> pl.DataFrame:\n",
    "    params = {}\n",
    "    for col in COLS:\n",
    "        mean = pl.col(col).shift().rolling_mean(n, min_periods=n).over(\"uid\")\n",
    "        std = pl.col(col).shift().rolling_std(n, min_periods=n).over(\"uid\")\n",
    "        params[col]=(pl.col(col) - mean).abs()/std\n",
    "    return df.sort(\"ts\").with_columns(**params).drop_nulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dacf316-fb27-44b1-bc1d-032c89bf583e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Testing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be366e6c-7f62-4888-a862-637617116193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "NUMBERS = [10000,100000,1000000]\n",
    "\n",
    "def save(n, path, engine):\n",
    "    idf = pd.DataFrame(dict(a=range(n),b=WHOLE_PERIOD))\n",
    "\n",
    "    with fa.engine_context(engine):\n",
    "        df = fa.transform(idf, make_dfs, Schema(\"ts:datetime\")+[(c,float) for c in COLS]+[(\"uid\",int)], partition=128)  \n",
    "        fa.save(df, path.format(n=n))\n",
    "\n",
    "class TestRunner:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def run_all(self):\n",
    "        for n in NUMBERS:\n",
    "            self.run(n)\n",
    "            \n",
    "    def pre(self):\n",
    "        pass\n",
    "    \n",
    "    def post(self):\n",
    "        pass\n",
    "    \n",
    "    def path(self, n):\n",
    "        pass\n",
    "\n",
    "    def run(self, n):\n",
    "        self.pre()\n",
    "        try:\n",
    "            with fa.engine_context(spark):\n",
    "                start = datetime.now()\n",
    "                self.func(self.path(n))\n",
    "                span = (datetime.now() - start).total_seconds()\n",
    "            print(f\"{self.func.__name__}({n}) --- {span} seconds\")\n",
    "        finally:\n",
    "            self.post()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4779ce07-0ae3-430c-b39c-f3e6b44f1e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for n in NUMBERS:\n",
    "    save(n, SPARK_PATH, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b7c4a3-02e9-423c-a99d-8423437431d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Databricks Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae15650-6e3f-4e56-93ca-5ee8ba59e491",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SparkTestRunner(TestRunner):\n",
    "    def pre(self):\n",
    "        spark.catalog.clearCache()\n",
    "    \n",
    "    def post(self):\n",
    "        pass\n",
    "    \n",
    "    def path(self, n):\n",
    "        return SPARK_PATH.format(n=n)\n",
    "    \n",
    "def spark_test(func):\n",
    "    return SparkTestRunner(func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "997872d3-407d-4b1a-ac4a-a1c22438be92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pure Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb0d686-6dad-478a-a4d7-908c81d38062",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "@spark_test\n",
    "def koalas_overhead(path):\n",
    "    print(pp.read_parquet(path).groupby(\"uid\").apply(lambda x:x.head(1))[\"_0\"].sum())\n",
    "\n",
    "@spark_test\n",
    "def pandas_udf_overhead(path):\n",
    "    df = spark.read.parquet(path)\n",
    "    df = df.groupby(\"uid\").applyInPandas(lambda x:x.head(1), schema=df.schema)\n",
    "    print(df.select(sum(df[\"_0\"])).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6847e0ee-e052-4a2d-8449-d77868e05772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4960.396094375464\n",
      "koalas_overhead(10000) --- 7.541973 seconds\n",
      "49938.184014258906\n",
      "koalas_overhead(100000) --- 40.058433 seconds\n",
      "499140.48774121556\n",
      "koalas_overhead(1000000) --- 346.786903 seconds\n"
     ]
    }
   ],
   "source": [
    "koalas_overhead.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bb4308-0eb2-486f-9232-6879a25920aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sum(_0)\n",
      "0  4960.396094\n",
      "pandas_udf_overhead(10000) --- 2.082695 seconds\n",
      "        sum(_0)\n",
      "0  49938.184014\n",
      "pandas_udf_overhead(100000) --- 9.210402 seconds\n",
      "        sum(_0)\n",
      "0  499139.67806\n",
      "pandas_udf_overhead(1000000) --- 73.292321 seconds\n"
     ]
    }
   ],
   "source": [
    "pandas_udf_overhead.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07d4731c-6e51-47d3-9f93-4dd9833ea9e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ZScore - Pandas on Spark \n",
    "\n",
    "Pandas on Spark needs special type annotation on output, we have to create a pandas dataframe in order to construct the schema..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67856287-3489-4fd7-9bdb-3c5f904c10f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as pp\n",
    "\n",
    "edf = make_df(0,\"2022-01-01\",20,\"D\")\n",
    "\n",
    "# schema:*\n",
    "def zscore_pp(df:pd.DataFrame, n) -> pp.DataFrame[zip(edf.columns, edf.dtypes)]:\n",
    "    return zscore_pd(df, n)\n",
    "\n",
    "@spark_test\n",
    "def koalas_zscore(path):\n",
    "    print(pp.read_parquet(path).groupby(\"uid\").apply(zscore_pp,n=PERIOD)[\"_0\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e729c611-a8b9-414e-be94-600aa893fdbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4378032.511842889\n",
      "koalas_zscore(10000) --- 49.886775 seconds\n"
     ]
    }
   ],
   "source": [
    "# this is to warmup the cluster\n",
    "koalas_zscore.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb5d0ac-cd0c-4319-980c-cbedb802f355",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4378032.511842885\n",
      "koalas_zscore(10000) --- 8.447558 seconds\n",
      "43808623.11610298\n",
      "koalas_zscore(100000) --- 64.523119 seconds\n",
      "438117632.98056763\n",
      "koalas_zscore(1000000) --- 437.265959 seconds\n"
     ]
    }
   ],
   "source": [
    "koalas_zscore.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c3fe641-aa3c-442e-b5c6-96402d9c382d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ZScore - Fugue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "114cd7c2-43f5-4548-b700-654da75219c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fugue.api as fa\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "@spark_test\n",
    "def zscore_pandas(path):\n",
    "    df = fa.transform(\n",
    "        path,\n",
    "        zscore_pd,\n",
    "        partition=\"uid\",\n",
    "        params=dict(n=PERIOD)\n",
    "    )\n",
    "    print(df.select(sum(df[\"_0\"])).toPandas())\n",
    "   \n",
    "@spark_test\n",
    "def zscore_pandas_coarse(path):\n",
    "    df = fa.transform(\n",
    "        path,\n",
    "        zscore_pd_gp,\n",
    "        partition=dict(by=\"uid\", algo=\"coarse\"),\n",
    "        params=dict(n=PERIOD)\n",
    "    )\n",
    "    print(df.select(sum(df[\"_0\"])).toPandas())\n",
    "\n",
    "@spark_test\n",
    "def zscore_polars(path):\n",
    "    df = fa.transform(\n",
    "        path,\n",
    "        zscore_pl,\n",
    "        partition=\"uid\",\n",
    "        params=dict(n=PERIOD)\n",
    "    )\n",
    "    print(df.select(sum(df[\"_0\"])).toPandas())\n",
    "\n",
    "@spark_test\n",
    "def zscore_polars_coarse(path):\n",
    "    df = fa.transform(\n",
    "        path,\n",
    "        zscore_pl_gp,\n",
    "        partition=dict(by=\"uid\", algo=\"coarse\"),\n",
    "        params=dict(n=PERIOD)\n",
    "    )\n",
    "    print(df.select(sum(df[\"_0\"])).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0605669-9633-4399-aa13-9ed2e8bf397a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sum(_0)\n",
      "0  4.378033e+06\n",
      "zscore_pandas(10000) --- 5.519285 seconds\n",
      "        sum(_0)\n",
      "0  4.380862e+07\n",
      "zscore_pandas(100000) --- 25.438292 seconds\n",
      "        sum(_0)\n",
      "0  4.381177e+08\n",
      "zscore_pandas(1000000) --- 194.85018 seconds\n"
     ]
    }
   ],
   "source": [
    "zscore_pandas.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25d864c-a66c-42cc-add3-6bb5dd800c80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sum(_0)\n",
      "0  4.378033e+06\n",
      "zscore_pandas_coarse(10000) --- 2.361903 seconds\n",
      "        sum(_0)\n",
      "0  4.380862e+07\n",
      "zscore_pandas_coarse(100000) --- 4.726595 seconds\n",
      "        sum(_0)\n",
      "0  4.381176e+08\n",
      "zscore_pandas_coarse(1000000) --- 37.439444 seconds\n"
     ]
    }
   ],
   "source": [
    "zscore_pandas_coarse.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d45eb8-6bee-4b80-9d54-66d6b1bdd4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sum(_0)\n",
      "0  4.378033e+06\n",
      "zscore_polars(10000) --- 3.59433 seconds\n",
      "        sum(_0)\n",
      "0  4.380862e+07\n",
      "zscore_polars(100000) --- 17.652728 seconds\n",
      "        sum(_0)\n",
      "0  4.381176e+08\n",
      "zscore_polars(1000000) --- 144.548372 seconds\n"
     ]
    }
   ],
   "source": [
    "zscore_polars.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5380d02-dcdb-423a-aee7-2e6ab9f0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sum(_0)\n",
      "0  4.378033e+06\n",
      "zscore_polars_coarse(10000) --- 1.617953 seconds\n",
      "        sum(_0)\n",
      "0  4.380862e+07\n",
      "zscore_polars_coarse(100000) --- 4.739778 seconds\n",
      "        sum(_0)\n",
      "0  4.381176e+08\n",
      "zscore_polars_coarse(1000000) --- 45.711359 seconds\n"
     ]
    }
   ],
   "source": [
    "zscore_polars_coarse.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe90df61-bc19-4433-b8b7-9bda7a193333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "WITH \n",
    "    mean_std AS (\n",
    "        SELECT \n",
    "            uid, ts,_0,_1,_2,_3,_4,_5,_6,_7,_8,_9,\n",
    "            AVG(_0) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_0,\n",
    "            STDDEV(_0) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_0,\n",
    "            AVG(_1) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_1,\n",
    "            STDDEV(_1) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_1,\n",
    "            AVG(_2) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_2,\n",
    "            STDDEV(_2) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_2,\n",
    "            AVG(_3) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_3,\n",
    "            STDDEV(_3) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_3,\n",
    "            AVG(_4) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_4,\n",
    "            STDDEV(_4) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_4,\n",
    "            AVG(_5) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_5,\n",
    "            STDDEV(_5) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_5,\n",
    "            AVG(_6) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_6,\n",
    "            STDDEV(_6) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_6,\n",
    "            AVG(_7) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_7,\n",
    "            STDDEV(_7) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_7,\n",
    "            AVG(_8) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_8,\n",
    "            STDDEV(_8) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_8,\n",
    "            AVG(_9) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS mean_9,\n",
    "            STDDEV(_9) OVER (PARTITION BY uid ORDER BY ts ROWS BETWEEN {PERIOD} PRECEDING AND 1 PRECEDING) AS std_9,\n",
    "            ROW_NUMBER() OVER (PARTITION BY uid ORDER BY ts) AS rn\n",
    "        FROM parquet.`{path}`\n",
    "    ),\n",
    "    z AS (\n",
    "        SELECT \n",
    "            uid, ts,\n",
    "            abs((_0 - mean_0)/std_0) AS z_0,\n",
    "            abs((_1 - mean_1)/std_1) AS z_1,\n",
    "            abs((_2 - mean_2)/std_2) AS z_2,\n",
    "            abs((_3 - mean_3)/std_3) AS z_3,\n",
    "            abs((_4 - mean_4)/std_4) AS z_4,\n",
    "            abs((_5 - mean_5)/std_5) AS z_5,\n",
    "            abs((_6 - mean_6)/std_6) AS z_6,\n",
    "            abs((_7 - mean_7)/std_7) AS z_7,\n",
    "            abs((_8 - mean_8)/std_8) AS z_8,\n",
    "            abs((_9 - mean_9)/std_9) AS z_9\n",
    "        FROM mean_std\n",
    "        WHERE rn>{PERIOD} AND mean_0 IS NOT NULL AND std_0 IS NOT NULL\n",
    "    )\n",
    "SELECT\n",
    "    SUM(z_0) AS z_0,\n",
    "    SUM(z_1) AS z_1,\n",
    "    SUM(z_2) AS z_2,\n",
    "    SUM(z_3) AS z_3,\n",
    "    SUM(z_4) AS z_4,\n",
    "    SUM(z_0) AS z_5,\n",
    "    SUM(z_5) AS z_6,\n",
    "    SUM(z_6) AS z_7,\n",
    "    SUM(z_7) AS z_8,\n",
    "    SUM(z_8) AS z_9\n",
    "FROM z\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3013529-f2b0-4da3-b80e-bc148ef56b25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@spark_test\n",
    "def zscore_sql(path):\n",
    "    df = spark.sql(sql.format(path=path, PERIOD=PERIOD))\n",
    "    print(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a81d0e9-a315-411d-b16e-9594cf3c571b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            z_0           z_1           z_2           z_3           z_4  \\\n",
      "0  4.378033e+06  4.380733e+06  4.384808e+06  4.378741e+06  4.380608e+06   \n",
      "\n",
      "            z_5           z_6           z_7           z_8           z_9  \n",
      "0  4.378033e+06  4.380569e+06  4.383062e+06  4.380676e+06  4.382290e+06  \n",
      "zscore_sql(10000) --- 21.696104 seconds\n",
      "            z_0           z_1           z_2           z_3           z_4  \\\n",
      "0  4.380862e+07  4.381683e+07  4.381767e+07  4.382026e+07  4.379173e+07   \n",
      "\n",
      "            z_5           z_6           z_7           z_8           z_9  \n",
      "0  4.380862e+07  4.381301e+07  4.382848e+07  4.381101e+07  4.381253e+07  \n",
      "zscore_sql(100000) --- 165.218836 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "zscore_sql.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4415c3a7-7efa-4e93-9e46-435187a215d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fugue vs Koalas",
   "notebookOrigID": 4348193075091125,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
